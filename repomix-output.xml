This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, content has been compressed (code blocks are separated by ‚ãÆ---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ‚ãÆ---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
ConsoleSearch/
  App.cs
  Config.cs
  ConsoleSearch.csproj
  DatabaseSqlite.cs
  DocumentHit.cs
  IDatabase.cs
  PatternDocumentHit.cs
  PatternSearchResult.cs
  Program.cs
  SearchLogic.cs
  SearchResult.cs
Documents/
  Modul 2 - Agenda/
    Assignemnts day 1.txt
    Database_Inspection_Guide.md
    Intro to Case.txt
    Test Data explanation.txt
  Modul 3 - Agenda/
    L√¶selektier.txt
    Opgaver_Modul3.txt
indexer/
  App.cs
  Config.cs
  Crawler.cs
  DatabaseSqlite.cs
  IDatabase.cs
  indexer.csproj
  Program.cs
  Renamer.cs
Shared/
  Model/
    BEDocument.cs
  Paths.cs
  Shared.csproj
.gitattributes
.gitignore
.repomixignore
assignments.md
Claude.md
README.txt
SearchEngine.sln
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(dotnet --version)",
      "Bash(dotnet build:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="ConsoleSearch/App.cs">
namespace ConsoleSearch
‚ãÆ----
public class App
‚ãÆ----
public void Run()
‚ãÆ----
SearchLogic mSearchLogic = new SearchLogic(new DatabaseSqlite());
Console.WriteLine("Console Search");
‚ãÆ----
string input = Console.ReadLine();
if (string.IsNullOrEmpty(input)) continue;
// Quit
if (input.Equals("q", StringComparison.OrdinalIgnoreCase)) break;
// Help (accept both '?' and optional '6')
‚ãÆ----
Console.Write("Enter new result limit (e.g., 15, or 'all'): ");
string limitInput = Console.ReadLine();
if (!string.IsNullOrEmpty(limitInput))
‚ãÆ----
if (limitInput.Equals("all", StringComparison.OrdinalIgnoreCase))
‚ãÆ----
else if (int.TryParse(limitInput, out int limit) && limit > 0)
‚ãÆ----
Console.WriteLine("Invalid input. Limit unchanged.");
‚ãÆ----
var patternResult = mSearchLogic.PatternSearch(input);
Console.WriteLine("Pattern Search Results:");
‚ãÆ----
string fileName = System.IO.Path.GetFileName(hit.Document.mUrl);
‚ãÆ----
Console.WriteLine($"{patternIdx}. {fileName} ({hit.MatchingWords.Count} {matchText}): {string.Join(", ", hit.MatchingWords)}");
‚ãÆ----
Console.WriteLine($"{patternIdx}: {hit.Document.mUrl} -- contains {hit.MatchingWords.Count} matching terms:");
Console.WriteLine($"    {string.Join(", ", hit.MatchingWords)}");
‚ãÆ----
Console.WriteLine($"Found {patternResult.Hits.Count} documents.");
‚ãÆ----
else if (input.StartsWith("/"))
‚ãÆ----
var query = input.Split(" ", StringSplitOptions.RemoveEmptyEntries);
var searchResult = mSearchLogic.Search(query);
‚ãÆ----
Console.WriteLine($"Ignored: {string.Join(',', searchResult.Ignored)}");
‚ãÆ----
string fileName = System.IO.Path.GetFileName(doc.Document.mUrl);
‚ãÆ----
// Get the matching terms (all query terms minus missing ones)
var allTerms = query.ToList();
var matchingTerms = allTerms.Except(doc.Missing).ToArray();
Console.WriteLine($"{searchIdx}. {fileName} ({doc.NoOfHits} {hitText}): {string.Join(", ", matchingTerms)}");
‚ãÆ----
Console.WriteLine($"{searchIdx} : {doc.Document.mUrl} -- contains {doc.NoOfHits} search terms");
‚ãÆ----
Console.WriteLine("    Index time: " + doc.Document.mIdxTime);
‚ãÆ----
Console.WriteLine($"    Missing: {ArrayAsString(doc.Missing.ToArray())}");
‚ãÆ----
Console.WriteLine("Documents: " + searchResult.Hits + ". Time: " + searchResult.TimeUsed.TotalMilliseconds);
‚ãÆ----
private void DisplayMenu()
‚ãÆ----
Console.WriteLine("\n----------------------------------------------------------------");
‚ãÆ----
string limitStatus = Config.ResultLimit.HasValue ? Config.ResultLimit.Value.ToString() : "ALL";
‚ãÆ----
Console.WriteLine($"OPTIONS: [1] Case Sensitive: {caseStatus}   [2] Timestamps: {timeStatus}   [3] Result Limit: {limitStatus}   [4] Pattern Search: {patternStatus}   [5] Compact View: {compactStatus}");
Console.WriteLine("Enter a search query, a number to toggle/change, '?' for help, or 'q' to quit.");
Console.Write("> ");
‚ãÆ----
private void DisplayHelp()
‚ãÆ----
Console.WriteLine();
Console.WriteLine("=== SEARCH ENGINE HELP ===");
‚ãÆ----
Console.WriteLine("CURRENT SETTINGS:");
// Case Sensitive
Console.WriteLine($"Case Sensitive: {(Config.CaseSensitive ? "ON" : "OFF")} - When ON, letter casing must match exactly.");
Console.WriteLine($"  Example: Query 'Hello' {(Config.CaseSensitive ? "will NOT match 'hello'" : "will match 'hello'")} in documents.");
Console.WriteLine("  Toggle via: [1] or /casesensitive=on|off");
‚ãÆ----
// Timestamps
Console.WriteLine($"Timestamps: {(Config.ViewTimeStamps ? "ON" : "OFF")} - Shows document indexing time under each result.");
Console.WriteLine("  Example line: 'Index time: 2025-01-15 14:30:22'");
Console.WriteLine("  Toggle via: [2] or /timestamp=on|off");
‚ãÆ----
// Result Limit
‚ãÆ----
Console.WriteLine($"Result Limit: {limitStatus} - Maximum number of results displayed (ALL = no limit).");
Console.WriteLine("  Example: If set to 20, only top 20 matches are shown.");
Console.WriteLine("  Change via: [3] or /results=NUMBER|all");
‚ãÆ----
// Pattern Search
Console.WriteLine($"Pattern Search: {(Config.PatternSearch ? "ON" : "OFF")} - Enables wildcard matching using ? and *.");
Console.WriteLine("  ? matches exactly one character. * matches zero or more characters.");
Console.WriteLine("  Example pattern: en*gy?  -> matches 'energy1', 'enggy2', 'enxxxgyA'");
Console.WriteLine("  Toggle via: [4]" + " or /pattern=on|off");
‚ãÆ----
// Compact View
Console.WriteLine($"Compact View: {(Config.CompactView ? "ON" : "OFF")} - Shows clean, simplified search results.");
Console.WriteLine("  Removes long file paths and displays results on single lines.");
Console.WriteLine("  This will also hide timestamps regardless of that setting.");
Console.WriteLine("  Example: '1. file.txt (3 matches): energy, power, grid' instead of full paths");
Console.WriteLine("  Toggle via: [5] or /compact=on|off");
‚ãÆ----
// Other commands
Console.WriteLine("OTHER:");
Console.WriteLine("- '?' shows this help.");
Console.WriteLine("- 'q' quits the application.");
‚ãÆ----
Console.WriteLine("Press any key to return to the menu...");
Console.ReadKey(true);
‚ãÆ----
private void HandleCommand(string input)
‚ãÆ----
var parts = input.Split('=');
var command = parts[0].ToLower();
var value = parts.Length > 1 ? parts[1].ToLower() : "";
‚ãÆ----
Console.WriteLine($"Case sensitivity set to {Config.CaseSensitive}");
‚ãÆ----
Console.WriteLine($"Timestamp display set to {Config.ViewTimeStamps}");
‚ãÆ----
else if (int.TryParse(value, out int limit) && limit > 0) Config.ResultLimit = limit;
‚ãÆ----
Console.WriteLine($"Result limit set to {limitStatus}");
‚ãÆ----
Console.WriteLine($"Pattern search set to {(Config.PatternSearch ? "ON" : "OFF")}");
‚ãÆ----
Console.WriteLine($"Compact view set to {(Config.CompactView ? "ON" : "OFF")}");
‚ãÆ----
Console.WriteLine($"Unknown command: {command}");
‚ãÆ----
string ArrayAsString(string[] s)
‚ãÆ----
return s.Length == 0 ? "[]" : $"[{String.Join(',', s)}]";
</file>

<file path="ConsoleSearch/Config.cs">
namespace ConsoleSearch
‚ãÆ----
public static class Config
</file>

<file path="ConsoleSearch/ConsoleSearch.csproj">
<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net9.0</TargetFramework>
  </PropertyGroup>

  <ItemGroup>
    <None Remove="SQLitePCLRaw.core" />
    <None Remove="Microsoft.Data.Sqlite" />
  </ItemGroup>
  <ItemGroup>
    <PackageReference Include="Microsoft.Data.Sqlite" Version="8.0.1" />
  </ItemGroup>
  <ItemGroup>
    <ProjectReference Include="..\Shared\Shared.csproj" />
  </ItemGroup>
</Project>
</file>

<file path="ConsoleSearch/DatabaseSqlite.cs">
namespace ConsoleSearch
‚ãÆ----
public class DatabaseSqlite : IDatabase
‚ãÆ----
private SqliteConnection _connection;
‚ãÆ----
var connectionStringBuilder = new SqliteConnectionStringBuilder();
‚ãÆ----
_connection = new SqliteConnection(connectionStringBuilder.ConnectionString);
_connection.Open();
‚ãÆ----
private void Execute(string sql)
‚ãÆ----
var cmd = _connection.CreateCommand();
‚ãÆ----
cmd.ExecuteNonQuery();
‚ãÆ----
// key is the id of the document, the value is number of search words in the document
public List<KeyValuePair<int, int>> GetDocuments(List<int> wordIds)
‚ãÆ----
/* Example sql statement looking for doc id's that
               contain words with id 2 and 3
               SELECT docId, COUNT(wordId) as count
                 FROM Occ
                WHERE wordId in (2,3)
             GROUP BY docId
             ORDER BY COUNT(wordId) DESC 
             */
‚ãÆ----
var selectCmd = _connection.CreateCommand();
‚ãÆ----
using (var reader = selectCmd.ExecuteReader())
‚ãÆ----
while (reader.Read())
‚ãÆ----
var docId = reader.GetInt32(0);
var count = reader.GetInt32(1);
res.Add(new KeyValuePair<int, int>(docId, count));
‚ãÆ----
private string AsString(List<int> x) => $"({string.Join(',', x)})";
private Dictionary<string, int> GetAllWords()
‚ãÆ----
var id = reader.GetInt32(0);
var w = reader.GetString(1);
res.Add(w, id);
‚ãÆ----
public List<BEDocument> GetDocDetails(List<int> docIds)
‚ãÆ----
var url = reader.GetString(1);
var idxTime = reader.GetString(2);
var creationTime = reader.GetString(3);
res.Add(new BEDocument { mId = id, mUrl = url, mIdxTime = idxTime, mCreationTime = creationTime });
‚ãÆ----
/* Return a list of id's for words; all them among wordIds, but not present in the document
         */
public List<int> getMissing(int docId, List<int> wordIds)
‚ãÆ----
var wordId = reader.GetInt32(0);
present.Add(wordId);
‚ãÆ----
result.Remove(w);
‚ãÆ----
public List<string> WordsFromIds(List<int> wordIds)
‚ãÆ----
var wordId = reader.GetString(0);
result.Add(wordId);
‚ãÆ----
public List<int> GetWordIds(string[] query, out List<string> outIgnored)
‚ãÆ----
var command = _connection.CreateCommand();
‚ãÆ----
command.Parameters.AddWithValue("@name", aWord);
‚ãÆ----
using (var reader = command.ExecuteReader())
‚ãÆ----
if (reader.Read())
‚ãÆ----
res.Add(reader.GetInt32(0));
‚ãÆ----
ignored.Add(aWord);
‚ãÆ----
public List<string> GetWordsMatchingPattern(string pattern)
‚ãÆ----
// GLOB is case-sensitive and uses * and ? as wildcards, which matches the user's input format.
‚ãÆ----
command.Parameters.AddWithValue("@pattern", pattern); // Use the original pattern
‚ãÆ----
// LIKE is case-insensitive by default. It uses % and _.
var sqlPattern = pattern.Replace('?', '_').Replace('*', '%');
‚ãÆ----
command.Parameters.AddWithValue("@pattern", sqlPattern);
‚ãÆ----
res.Add(reader.GetString(0));
‚ãÆ----
public Dictionary<int, List<string>> GetDocsWithMatchingWords(List<string> matchingWords)
‚ãÆ----
// Step 1: Get word IDs and create lookup maps
‚ãÆ----
var wordParams = string.Join(",", matchingWords.Select((_, i) => $"@p{i}"));
‚ãÆ----
command.Parameters.AddWithValue($"@p{i}", matchingWords[i]);
‚ãÆ----
int id = reader.GetInt32(0);
string name = reader.GetString(1);
‚ãÆ----
// Step 2: Get all occurrences for the found word IDs
var idParams = string.Join(",", wordIdToName.Keys.Select((_, i) => $"@id{i}"));
var occCommand = _connection.CreateCommand();
‚ãÆ----
occCommand.Parameters.AddWithValue($"@id{j++}", id);
‚ãÆ----
// Step 3: Map docIds to the list of matching words they contain
using (var reader = occCommand.ExecuteReader())
‚ãÆ----
int docId = reader.GetInt32(0);
int wordId = reader.GetInt32(1);
if (!result.ContainsKey(docId))
‚ãÆ----
result[docId].Add(wordIdToName[wordId]);
</file>

<file path="ConsoleSearch/DocumentHit.cs">
namespace ConsoleSearch
‚ãÆ----
public class DocumentHit
</file>

<file path="ConsoleSearch/IDatabase.cs">
namespace ConsoleSearch
‚ãÆ----
public interface IDatabase
‚ãÆ----
/// <summary>
/// Get id's for words in [query]. [outIgnored] contains those word from query that is
/// not present in any document.
/// </summary>
List<int> GetWordIds(string[] query, out List<string> outIgnored);
List<BEDocument> GetDocDetails(List<int> docIds);
‚ãÆ----
/// Perform the essential search for documents. It will return
/// a list of KeyValuePairs - the key is the id of the
/// document, and value is the number of words from the query
/// contained in the document.
‚ãÆ----
List<KeyValuePair<int, int>> GetDocuments(List<int> wordIds);
‚ãÆ----
/// Return all id of words, contained in [wordIds], but not
/// present in the document with id [docId]
‚ãÆ----
List<int> getMissing(int docId, List<int> wordIds);
‚ãÆ----
/// Convert a list of word id's to a list of the value of the
/// words
‚ãÆ----
List<string> WordsFromIds(List<int> wordIds);
List<string> GetWordsMatchingPattern(string pattern);
Dictionary<int, List<string>> GetDocsWithMatchingWords(List<string> matchingWords);
</file>

<file path="ConsoleSearch/PatternDocumentHit.cs">
namespace ConsoleSearch
‚ãÆ----
public class PatternDocumentHit
</file>

<file path="ConsoleSearch/PatternSearchResult.cs">
namespace ConsoleSearch
‚ãÆ----
public class PatternSearchResult
</file>

<file path="ConsoleSearch/Program.cs">
namespace ConsoleSearch
‚ãÆ----
class Program
‚ãÆ----
static void Main(string[] args)
‚ãÆ----
new App().Run();
</file>

<file path="ConsoleSearch/SearchLogic.cs">
namespace ConsoleSearch
‚ãÆ----
public class SearchLogic
‚ãÆ----
IDatabase mDatabase;
‚ãÆ----
/* Perform search of documents containing words from query. The result will
         * contain details about amost maxAmount of documents.
         */
public SearchResult Search(String[] query)
‚ãÆ----
DateTime start = DateTime.Now;
// Convert words to wordids
var wordIds = mDatabase.GetWordIds(query, out ignored);
// perform the search - get all docIds
var docIds =  mDatabase.GetDocuments(wordIds);
// get ids for the first maxAmount
‚ãÆ----
int limit = Config.ResultLimit.HasValue ? Math.Min(Config.ResultLimit.Value, docIds.Count) : docIds.Count;
foreach (var p in docIds.GetRange(0, limit))
top.Add(p.Key);
// compose the result.
// all the documentHit
‚ãÆ----
foreach (var doc in mDatabase.GetDocDetails(top))
‚ãÆ----
var missing = mDatabase.WordsFromIds(mDatabase.getMissing(doc.mId, wordIds));
docresult.Add(new DocumentHit(doc, docIds[idx++].Value, missing));
‚ãÆ----
return new SearchResult(query, docIds.Count, docresult, ignored, DateTime.Now - start);
‚ãÆ----
public PatternSearchResult PatternSearch(string pattern)
‚ãÆ----
// Step 1: Find all words in the database that match the pattern
var matchingWords = mDatabase.GetWordsMatchingPattern(pattern);
‚ãÆ----
return new PatternSearchResult(new List<PatternDocumentHit>());
‚ãÆ----
// Step 2: Find which documents contain these words and which specific words are in each
var docsWithWords = mDatabase.GetDocsWithMatchingWords(matchingWords);
// Step 3: Apply the result limit BEFORE fetching full details
var limitedDocIds = docsWithWords.Keys.AsEnumerable();
‚ãÆ----
limitedDocIds = limitedDocIds.Take(Config.ResultLimit.Value);
‚ãÆ----
var finalDocIds = limitedDocIds.ToList();
// Step 4: Get the full details for the limited set of documents
var docDetails = mDatabase.GetDocDetails(finalDocIds);
var docDetailsMap = docDetails.ToDictionary(d => d.mId);
// Step 5: Assemble the final result
‚ãÆ----
foreach (var docId in finalDocIds) // Iterate over the limited list of IDs
‚ãÆ----
if (docDetailsMap.ContainsKey(docId))
‚ãÆ----
hits.Add(new PatternDocumentHit(docDetailsMap[docId], wordsInDoc));
‚ãÆ----
return new PatternSearchResult(hits);
</file>

<file path="ConsoleSearch/SearchResult.cs">
namespace ConsoleSearch
‚ãÆ----
/*
     * A data class representing the result of a search.
     * Hits is the total number of documents containing at least one word from the query.
     * DocumentHits is the documents and the number of words from the query contained in the document - see
     * the class DocumentHit
     * Ignored contains words from the query not present in the document base.
     * TimeUsed is the timespan used to perform the search.
     */
public class SearchResult
</file>

<file path="Documents/Modul 2 - Agenda/Assignemnts day 1.txt">
Agenda

1. Hvad er en s√∏gemaskine?

Indeksering
S√∏gning
Datamodellen
2. Opgave 1 l√∏ses.

4. Code walk-through

Arkitektur
Indexer - crawler
console search
5. Opgave 2, 3, ... l√∏ses. Hvis I har andre opgaver, som I er nysgerrige p√•, s√• er I mere end velkommen til at kaste jer over dem. Del gerne jeres ideer med mig inden I g√•r i krig. M√•ske jeg kan hj√¶lpe jer i den rigtige retning eller forhindre I fors√∏ger at l√∏se for stor en opgave.

 

Opgaver

Opgave 1
F√• s√∏gemaskinen til at k√∏re p√• jeres computer. I skal

Koden er sat op til at k√∏re via .net 9.0. S√∏rg for at det er installeret p√• jeres computer.
S√∏gemaskinen anvender sqlite som database. Installer en sqlite browser - se https://sqlitebrowser.org/dl/Links to an external site..
S√∏rge for at builde alle projekter og eventuelt opdatere nuget packages.
L√¶gge nogle dokumenter i folderen til indexering. I kan selv lave nogle eller bruge nogle af dem i filen seData.zip.
Opdatere Shared.Path s√• stien til databasen peger p√• databasen.
Opdatere Indexer.Config s√• stien til hvor filerne der skal indekeres ligger er korrekt.
K√∏r indexer
Inspic√©r databasen, check at alle dokumenter er indexeret og lav nogle stikpr√∏ver mht hvilke ord der er indexeret.
K√∏r searchConsole. Afpr√∏v med foresp√∏rgsler p√• f√∏rst 1 ord, s√• 2 ord og tilsidst flere end 2 ord. Check b√•de hit og ikke hit.
Opgave 2
Som det er nu, vil indekseren afslutte med at udskrive alle ord til konsollen. Output kan fx. se s√•ledes ud:

DONE! used 986905.577
Indexed 50850 documents
Number of different words: 292796
The first 10 is:
<Message, 1>
<ID, 2>
<6184043, 3>
<1075857585289, 4>
<JavaMail, 5>
<evans, 6>
<thyme, 7>
<Date, 8>
<Tue, 9>
<28, 10>

Heraf fremg√•r det at processen med at indeksere de cirka 50.000 dokumenter tog knap 1000 sekunder (godt 15 min) og at der blev fundet knap 300.000 forskellige ord.

Lav det om s√• indekseringen afsluttes med, at der informeres om hvor mange ord forekomster der er indekseret og der sp√∏rges til hvor mange ord man vil se. Derefter vises det √∏nskede antal med deres hyppighed, men de ord der forekommer hyppigst, skal vises f√∏rst. S√• sidste det af output kunne v√¶re:

<Message, 1> - 48342
<ID, 2> - 47234
<mail, 12> - 47124
<the, 45> - 46847
... 

Opgave 3
I versionen er s√∏gningen case sensitiv. S√∏rg for brugeren selv kan sl√• dette fra eller til. Det kan g√∏res ved, at ConsoleSearch ikke bare kan modtage queries, men ogs√• kommandoer, s√• brugeren kan indtaste "/casesensitive=on" hvis s√∏gningerne skal v√¶re case sensitive (som de er nu), og indtaste "/casesensitive=off", hvis s√∏gningerne ikke skal v√¶re case sensitive. Alternativt - eller til at starte med - kan I g√∏re det ved at oprette en Config klasse med en attribut med navn CaseSensitive af type boolean.

Opgave 4
N√•r man s√∏ger, f√•r man info om tidspunkt for indeksering af dokumenter. S√∏rg for at brugeren v√¶lger dette til eller fra. Det kan g√∏res ved brugeren kan skrive "/timestamp=on", hvis de skal med i resultatlisten eller "/timestamp=off", hvis det ikke skal med. Alternativt - eller til at starte med - kan I g√∏re det ved at have en attribut med navn ViewTimeStamps af type boolean i config.cs i Shared projektet.

Opgave 5
Som det er nu, vises de 10 "bedste" dokumenter som resultat af en s√∏gning. S√¶t dette til 20. Derefter, s√∏rg for at brugeren kan v√¶lge dette, fx ved kommandoen "/results=15" for at f√• de 15 bedste, eller "/results=all" for at f√• vist alle. Alternativt - eller til at starte med - kan I g√∏re det ved, at have en attribut i Config klassen, som rer√¶senterer denne mulige begr√¶nsning. En simpel type for dette kunne v√¶re int? - alts√• en int, som kan v√¶re null. Her kunne "reglen" s√• v√¶re, at hvis den er null, skal man se alle resultater, ellers er dens v√¶rdi gr√¶nsen for hvor mange resultater der skal vises.

Opgave 6 - sv√¶r - m√•ske i projektet?
Udvid s√∏gemaskinen med mulighed for m√∏nsters√∏gning. Forestil jer at dom√¶net er politi og at de i forbindelse med en forbrydelse har en del af en nummerplade, fx. de ved den starter med BJ og ender p√• 7. De vil nu gerne finde alle steder hvor dette nummer kan v√¶re anf√∏rt. Med s√•kaldte wildcard eller regul√¶re udtryk kunne et m√∏nster v√¶re f√∏lgende "BJ????7". Alle ord p√• 7 tegn, startende med BJ og slutter med 7 vil matche dette m√∏nster. I et m√∏nster kan bruges enten '?' som st√•r for netop et vilk√•rligt tegn, mens '*' st√•r for et vilk√•rligt antal (ogs√• ingen) tegn. Til at s√∏ge efter ovenn√¶vnte nummerplade, kunne man bruge et lidt simplere m√∏nster, nemlig "BJ*7". Men det vil matche ord som bestemt ikke angiver en nummerplade. Hvorfor det?
Resultatet af en m√∏nsters√∏gning skal v√¶re alle de dokumenter, som indeholder et ord, som matcher s√∏geordet. Derudover skal resultatet vise de ord fra dokumentet som der matches p√•. Derved kunne output komme til at se s√•ledes ud:

enter search terms - q for quit
BJ????7
Pattern Search
1: /Users/peter/documents/2019/obs/05-12.txt -- contains 3 matching terms:
BJ12347, BJERGE7, BJ73857

2: /Users/John/documents/2017/contacts/12_17.txt -- contains 1 matching term:
BJ43567
I den f√∏rste version af m√∏nsters√∏gning er det en god id√© at lave det, s√• der kun kan s√∏ges p√• et m√∏nster og ikke i kombination med almindelige s√∏geord.
</file>

<file path="Documents/Modul 2 - Agenda/Database_Inspection_Guide.md">
# Database Inspektion & S√∏getest Vejledning

Denne vejledning viser hvordan du inspicerer SQLite databasen og grundigt tester s√∏gefunktionaliteten.

## üìä Database Inspektion med SQLite Browser

### 1. √Öbn Databasen
1. Start **DB Browser for SQLite**
2. **File ‚Üí Open Database**
3. Naviger til: `/Users/rosell/ITA_SEM6_SearchEngine/Data/searchDB.db`
4. Klik **Open**

### 2. Verificer at Alle Dokumenter er Indekseret

#### Tjek Dokument Antal
1. Klik **Browse Data** fanen
2. V√¶lg **`document`** tabellen fra dropdown
3. Verificer at du ser **3.034 r√¶kker** (for medium datas√¶t)
4. Scroll gennem for at se forskellige dokument stier

#### Eksempel Dokument Indgange
Led efter indgange som:
```
id: 1, url: /Users/rosell/.../allen-p/all_documents/1.txt, idxTime: 8/27/2025 12:01:54 PM
id: 2, url: /Users/rosell/.../allen-p/all_documents/2.txt, idxTime: 8/27/2025 12:01:54 PM
```

### 3. Inspic√©r Indekserede Ord (Stikpr√∏ver)

#### Tjek Ord Antal
1. V√¶lg **`word`** tabellen fra dropdown  
2. Verificer at du ser **31.079 r√¶kker** (for medium datas√¶t)
3. Browse gennem ordene for at se hvad der blev indekseret

#### Eksempel Ord Indgange
Led efter indgange som:
```
id: 1, name: Message
id: 2, name: ID  
id: 5, name: JavaMail
id: 6, name: evans
```

#### Avanceret Ord Analyse med SQL
Klik **Execute SQL** fanen og k√∏r disse foresp√∏rgsler:

```sql
-- T√¶l totale dokumenter
SELECT COUNT(*) as total_documents FROM document;

-- T√¶l totale unikke ord
SELECT COUNT(*) as total_words FROM word;

-- T√¶l totale ord forekomster
SELECT COUNT(*) as total_occurrences FROM occ;

-- Find hyppigste ord
SELECT w.name, COUNT(*) as frequency
FROM word w
JOIN occ o ON w.id = o.wordId
GROUP BY w.name
ORDER BY frequency DESC
LIMIT 20;

-- Find dokumenter der indeholder specifikt ord
SELECT d.url, d.idxTime 
FROM document d
JOIN occ o ON d.id = o.docId  
JOIN word w ON o.wordId = w.id
WHERE w.name = 'energy'
ORDER BY d.idxTime;

-- Tjek om specifikke ord eksisterer
SELECT name FROM word 
WHERE name IN ('power', 'energy', 'meeting', 'email')
ORDER BY name;

-- Tilf√¶ldige dokumenter
SELECT * FROM document 
ORDER BY RANDOM() 
LIMIT 10;
```

### 4. Verificer Indeks Integritet

Tjek **`occ`** (occurrence) tabellen:
```sql
-- Verificer ord-dokument relationer
SELECT COUNT(*) as total_relationships FROM occ;

-- Eksempel ord-dokument forbindelser
SELECT w.name as word, d.url as document
FROM occ o
JOIN word w ON o.wordId = w.id
JOIN document d ON o.docId = d.id
LIMIT 20;
```

## üîç S√∏gekonsol Test

### 1. Start S√∏gekonsol
```bash
cd /Users/rosell/ITA_SEM6_SearchEngine/ConsoleSearch
dotnet run
```

### 2. Test Enkelt Ord Foresp√∏rgsler (1 ord)

#### Forventede Hit:
```
energy
```
**Forventet:** Flere resultater der viser dokumenter indeholdende "energy"

```
power
```
**Forventet:** Flere resultater fra str√∏m-relaterede emails

```
meeting
```
**Forventet:** M√∏de-relaterede dokumenter

#### Forventede Ingen Hit:
```
unicorn
```
**Forventet:** "Ignored: unicorn" og "Documents: 0"

```
supercalifragilisticexpialidocious
```
**Forventet:** "Ignored: [ord]" og "Documents: 0"

### 3. Test To Ord Foresp√∏rgsler (2 ord)

#### Begge Ord Fundet:
```
power plant
```
**Forventet:** Dokumenter indeholdende b√•de "power" OG "plant", rangeret efter relevans

```
energy meeting
```
**Forventet:** Dokumenter indeholdende begge ord

#### Et Ord Mangler:
```
power unicorn
```
**Forventet:** 
- "Ignored: unicorn"
- Resultater indeholdende kun "power"
- "Missing: [unicorn]" i resultat detaljer

### 4. Test Multiple Ord Foresp√∏rgsler (flere end 2 ord)

#### Alle Ord Fundet:
```
power energy meeting
```
**Forventet:** Dokumenter indeholdende alle tre termer (h√∏jeste score f√∏rst)

#### Blandede Resultater:
```
power energy unicorn dragon
```
**Forventet:**
- "Ignored: unicorn, dragon"
- Resultater med "power" og "energy"
- "Missing:" felt viser hvilke termer der mangler fra hvert dokument

#### Alle Ord Mangler:
```
unicorn dragon phoenix
```
**Forventet:** "Ignored: unicorn, dragon, phoenix" og "Documents: 0"

### 5. Forventet Output Format

For hvert s√∏geresultat skal du se:
```
1 : /sti/til/dokument.txt -- contains X search terms
Index time: 8/27/2025 12:01:54 PM
Missing: [liste af manglende termer]
```

Plus sammendrag:
```
Documents: 159. Time: 76.5
```

## üìã Verifikations Tjekliste

### Database Integritet ‚úì
- [ ] Document tabel har 3.034 indgange (medium datas√¶t)
- [ ] Word tabel har ~31.079 indgange  
- [ ] Occurrence tabel har mange relationer
- [ ] Filstier i document tabel er korrekte
- [ ] Indeks tidsstempler er nylige

### S√∏gefunktionalitet ‚úì
- [ ] Enkelt ord s√∏gning returnerer resultater
- [ ] Enkelt ord s√∏gning h√•ndterer "ingen resultater" elegant
- [ ] To ord s√∏gning rangerer efter relevans (2 match > 1 match)
- [ ] Multiple ord s√∏gning viser "Missing:" termer korrekt
- [ ] Ikke-eksisterende ord viser "Ignored:" besked
- [ ] Foresp√∏rgselstid er rimelig (< 200ms for de fleste foresp√∏rgsler)
- [ ] Resultater er begr√¶nset til top 10
- [ ] Resultater viser filstier, match antal og tidsstempler

## üéØ Eksempel Test Session

```
Console Search
enter search terms - q for quit

energy
[Skal vise ~75 resultater]

power plant  
[Skal vise dokumenter med begge termer f√∏rst]

meeting email schedule
[Skal vise dokumenter rangeret efter hvor mange termer de indeholder]

unicorn
Ignored: unicorn
Documents: 0. Time: 46.615

q
[Afslut]
```

## üí° Tips

1. **Tilf√¶ldig Pr√∏vetagning**: Brug SQL foresp√∏rgsler til at v√¶lge tilf√¶ldige dokumenter og verificer de er s√∏gbare
2. **Ord Verifikation**: V√¶lg ord fra word tabellen og verificer de returnerer s√∏geresultater  
3. **Performance**: Bem√¶rk s√∏getider - de skal v√¶re under 100ms for simple foresp√∏rgsler
4. **Edge Cases**: Test tomme foresp√∏rgsler, meget lange foresp√∏rgsler og specielle tegn
5. **Konsistens**: K√∏r den samme foresp√∏rgsel flere gange for at verificere konsistente resultater

Dette fuldf√∏rer database inspektion og s√∏getest kravene fra Opgave 1.
</file>

<file path="Documents/Modul 2 - Agenda/Intro to Case.txt">
Intro til case ‚Äì en intern s√∏gemaskine
Kontekst og krav
Brugerfladen
Indeksering og datamodel
Arkitektur
Walking Skeleton

Kontekst
En virksomhed/organisation med 50++ ansatte, som arbejder med og producerer dokumenter
Dokumenter indeholder ustruktureret data og findes i mange formater (docx, pdf, rtf, txt, csv, pptx, xlxs osv.)
Den samlede dokument samling er allokeret p√• mange fil servere p√• tilsammen flere terra-bytes.
Behovet er et s√∏gesystem, som tilbyder ‚Äúinstant content search‚Äù og som kan tilpasses dom√¶net.

Brugergr√¶nsefladen
Brugergr√¶nsefladen kan v√¶re en web applikation eller en desktop applikation.
Web applikationen vil v√¶re begr√¶nset til intranettet.
En desktop applikation skal installeres p√• alle computere.
Brugeren vil have en Google-like brugerflade, hvor 
der indtastes s√∏geord (query)
Resultatet er en liste af hits med titel, link, dato, score og snippet.


Specielle Krav
100% recall ‚Äì det betyder, at alle dokumenter, som indeholder mindst et af s√∏geordene, skal med i resultatet.
Rankeringen skal v√¶re med aftagene score.
Givet en s√∏gning og et dokument, s√• er scoren et tal mellem 0 og 1. 
0 hvis ingen af s√∏geordene er i dokumentet
1 hvis alle s√∏geordene er i dokumentet.
Hvis x% af s√∏geordene er i dokumentet, s√• er scoren x/100.

Tilpasse dom√¶ne
Hvert dom√¶ne (ingeni√∏r, forsikring, politi, meteologi‚Ä¶) har nogle specifikke betegnelser/slang.
Det skal v√¶re muligt at s√∏ge efter (dom√¶ne specifikke) synonymer.
Dette kan g√∏res ved, at s√∏gemaskinen g√∏r brug af en synonym ordbog (termnet), s√•ledes at der ogs√• s√∏ges p√• synonymer.
Det konkrete indhold af synonym ordbogen kan vedligeholdes af brugerne (dom√¶net).


Indeksering
For at kunne udf√∏re indholdss√∏gning i millioner af dokumenter effektivt, kan man indeksere dokumenterne.
En indekser genneml√¶ser dokumenter (eng: crawl) og for hvert af disse vil det udtr√¶kke ord og danne et omvendt indeks.
Et omvendt indeks tilbyder effektiv s√∏gning for dokumenter som indeholder bestemte ord.

En begrebsm√¶ssig datamodel for det omvendte indeks.
Document
Title
Link
Date
Word
value
Occurrence
*
*
Filsystemet kan hurtigt svare p√• hvilke ord et givet dokument indeholder. 
Behovet er dog hurtige svar p√• det omvendte: 

                          Hvilke dokumenter indeholder et givet ord?

Walking Skeleton (version 1)Funktionalitet
S√∏gningen
er et konsol program indeholdende s√∏gelogikken
Ingen snippets
Ingen synonym ordbog
Indekser
er et konsol program og kan indeksere bare en top-folder 
Kan kun indeksere .txt dokumenter
kan ikke opdatere ‚Äì laver helt nyt indeks n√•r den startes
Databasen
En simpel SQLite database med 3 tabeller: Document, Word, og Occurrence.

En relationel datamodel for det omvendte indeks
Document
docId
Title
Link
Date
Term
termId
value
Occurrence
docId
termId

Walking Skeleton (version 1)Arkitektur ‚Äì class diagram

Walking Skeleton (version 1)Arkitektur - deployment

Arkitektur ‚Äì version 2
Search programDesktop
Search Engine(API)
Database
Indexer
File system
Search programWebApp

Class diagram ‚Äì version 2
</file>

<file path="Documents/Modul 2 - Agenda/Test Data explanation.txt">
Testdata
En meget vigtig del af udviklingen af s√∏gemaskinen er, at have styr p√• sine test data. Uden dem, kan man slet ikke afpr√∏ve hvordan den virker i produktion.

Til s√∏gemaskinen har man brug for mange filer med et realistisk indhold. Til det form√•l er enronLinks to an external site. datas√¶ttet helt perfekt, og er da ogs√• blevet brugt til rigtig meget. Det er et snapshot af en mail-server, hvor hver mail er skrevet i en fil, og hvor hver mailbox er en folder. Ligeledes er hver bruger ogs√• en folder. Se figur herunder.

Screenshot 2024-08-30 at 10.56.41.png

 

Filstruktur for mailserver

Der er lavet tre datas√¶t i files seData.zip, som kan hentes her Download her(fylder 100mb). De tre datas√¶t:
** SE MIG **
// Jeg har gjort dette, alt ligger i: C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data

small - et lille antal (13) mails i en folder. Kan bruges til funktionel test.
Medium - cirka 5000 mails i cirka 20 mailbokse for en bruger. Kan bruges til funktionel test - og en start p√• performance test.
Large - cirka 50000 mails for 15 brugere. Kan bruges til performance test.
Derudover er selvf√∏lgelig ogs√• hele datas√¶ttet fra enron som indeholder cirka 500.000 mails.
</file>

<file path="Documents/Modul 3 - Agenda/L√¶selektier.txt">
AKF Scale Cube - Skaleringsarkitektur
Kapitel 22: Introduktion til AKF Scale Cube
Grundl√¶ggende koncept
AKF Scale Cube er en 3-dimensionel model til skalering af systemer, organisationer og processer. Modellen starter fra et monolitisk udgangspunkt (0,0,0) - typisk et enkelt system p√• en enkelt server - og ekspanderer langs tre akser, hver repr√¶senterende en unik skaleringsmetode.
  

De tre skalerings-dimensioner
X-aksen: Kloning og replikering
* Princip: Kloning af services/data uden bias - arbejde distribueres ligeligt mellem identiske kopier
* Fordele: Simpel implementering, lav omkostning, hurtig udrulning
* Ulemper: Skalerer ikke godt med voksende datam√¶ngder eller instruktionss√¶t
* Anvendelse: Ideel som f√∏rste skridt i skalering n√•r transaktionsvolumen vokser
Y-aksen: Funktionel opdeling
* Princip: Opdeling efter ansvar, handling eller datatype (service-orienterede splits)
* Fordele: H√•ndterer kompleksitet, reducerer instruktionss√¶t, forbedrer fejlisolering
* Ulemper: H√∏jere implementeringsomkostning end x-akse, kr√¶ver redesign
* Anvendelse: N√∏dvendig n√•r monolitiske systemer bliver for komplekse
Z-aksen: Kunde-segmentering
* Princip: Opdeling baseret p√• kunde/anmoder med bias mod specifikke brugergrupper
* Fordele: Fremragende skalerbarhed, geografisk distribution mulig, kundespecifik optimering
* Ulemper: Ofte dyreste at implementere, kr√¶ver lookup-mekanismer
* Anvendelse: Kritisk for virksomheder med hurtig kundev√¶kst
  

Kapitel 23: Splitting Applications for Scale
Applikations-specifik implementering
N√•r AKF Scale Cube anvendes p√• applikationer, f√•r hver akse mere specifik betydning:
  

X-akse for applikationer
* Horisontal duplikering/kloning af services
* Load balancing mellem identiske instanser
* Ideel til h√•ndtering af transaktionsv√¶kst
* Udfordringer med stateful applikationer kr√¶ver session management
Y-akse for applikationer
* Split efter funktion eller service (verb-baseret opdeling)
* Adresserer monolitisk arkitektur gennem microservices
* Forbedrer udviklingshastighed og team-specialisering
* Muligg√∏r uafh√¶ngig skalering af forskellige funktioner
Z-akse for applikationer
* Kunde/anmoder-orienterede splits med lookup-baseret routing
* Fremragende til kundev√¶kst og data-partitionering
* Muligg√∏r differentierede serviceniveauer (fx freemium modeller)
* Underst√∏tter geografisk distribution af services
Praktisk integration af akser
Kombinationen af akser skaber robust og skalerbar arkitektur. Nedenst√•ende eksempel viser hvordan kunde-segmentering (z-akse) kombineres med redundans (x-akse) for at opn√• b√•de skalerbarhed og h√∏j tilg√¶ngelighed:
  

Hver kundegruppe (A-F, G-N, O-Z) har sin egen pod med multiple instanser, hvilket sikrer b√•de performance og tilg√¶ngelighed.
Real-world implementering: PROS Airline System
PROS systemet demonstrerer kraftfuld anvendelse af alle tre akser i et komplekst produktionsmilj√∏:
  

Y-akse splits i PROS
* Origin & Destination (O/D) model separeret fra Real-Time Dynamic Pricing (RTDP)
* Inventory Stream Service som selvst√¶ndig komponent
* Cache Distributor h√•ndterer data-distribution
Z-akse splits i PROS
* Forskellige Global Distribution Systems (GDS) betjenes af dedikerede RTDP instanser
* Hver GDS f√•r sin egen "swim lane" for fault isolation
X-akse splits i PROS
* DB Clusters med N1-N5 noder for redundans
* Multiple Availability Services
* Load-balancerede instanser inden for hver service
Integration og Best Practices
Design vs. Implementering
* Design for alle tre akser tidligt i produktudviklingen
* Implementer kun n√•r forretningsbehov kr√¶ver det
* Start typisk med x-akse, tilf√∏j y- og z-akse efter behov
Hierarkisk struktur
* X-akse ofte underordnet Y eller Z
* Y- og Z-akse kan v√¶re prim√¶re splits med x-akse for redundans
* V√¶lg prim√¶r akse baseret p√• st√∏rste skalerings-udfordring
Skalerings-strategi
1. Transaktionsv√¶kst: Start med x-akse kloning
2. Kompleksitetsv√¶kst: Tilf√∏j y-akse service splits
3. Kundev√¶kst: Implementer z-akse segmentering
4. Kombiner efter behov: Brug alle tre for "near infinite scale"
N√∏glepunkter
X-akse karakteristika
* Styrker: Simpel, billig, hurtig at implementere
* Svagheder: Begr√¶nset af data- og kodekompleksitet
* Use case: F√∏rste skridt i skalering, transaktionsvolumen
Y-akse karakteristika
* Styrker: H√•ndterer kompleksitet, forbedrer teamproduktivitet
* Svagheder: Kr√¶ver arkitektur√¶ndringer, dyrere end x-akse
* Use case: Feature-rige applikationer, microservices
Z-akse karakteristika
* Styrker: Ekstrem skalerbarhed, kundespecifik optimering
* Svagheder: Mest kompleks og dyr at implementere
* Use case: Hurtig kundev√¶kst, geografisk distribution
Opsummering
AKF Scale Cube tilbyder en struktureret og konceptuel ramme for systemskalering gennem tre komplement√¶re dimensioner. X-aksen h√•ndterer transaktionsvolumen gennem simpel replikering, Y-aksen adresserer kompleksitet gennem funktionel opdeling, og Z-aksen muligg√∏r ekstrem skalering gennem kunde-segmentering.
Succesfuld anvendelse kr√¶ver forst√•else for at hver akse l√∏ser forskellige skalerings-udfordringer og har forskellige omkostningsprofiler. Ved at designe for alle tre dimensioner tidligt, men implementere progressivt baseret p√• faktiske behov, kan organisationer opn√• effektiv skalering uden overinvestering i infrastruktur.
PROS eksemplet illustrerer hvordan moderne systemer kan h√•ndtere ekstreme krav til b√•de volumen og tilg√¶ngelighed gennem intelligent kombination af alle tre skaleringsmetoder, resulterende i systemer der kan betjene millioner af transaktioner med h√∏j p√•lidelighed.
Vigtige l√¶ringspunkter
* Design tidligt, implementer sent: Planl√¶g for alle tre akser, men byg kun hvad der er n√∏dvendigt
* Start simpelt: X-akse er ofte det bedste f√∏rste skridt
* Kombiner strategisk: Forskellige akser l√∏ser forskellige problemer
* Fokus p√• behov: Lad faktiske skalerings-udfordringer drive implementering
* M√•l og juster: Brug produktionsdata til at informere skalerings-beslutninger


________________
</file>

<file path="Documents/Modul 3 - Agenda/Opgaver_Modul3.txt">
# Opgaver Modul 3 - AKF Scale Cube
## IT-Arkitektur Semester 6, Erhvervsakademiet Aarhus

---

## Opgave 1: Webshop Skalering

**Forestil jer en webshop, hvor indk√∏bskurven (som er tilstand) ‚Äì ligger p√• web-serveren. Webshoppen performer d√•rligt og man t√¶nker p√• at x-skalere webshoppen.**

### Problemer:
- **Session Affinity Problem**: N√•r indk√∏bskurven ligger p√• web-serveren, skal brugeren altid ramme samme server-instans. Dette eliminerer fordelene ved X-akse skalering.
- **Load Balancer Kompleksitet**: Kr√¶ver "sticky sessions" eller session affinity, hvilket komplicerer load balancing og kan skabe ubalance.
- **Single Point of Failure**: Hvis en server g√•r ned, mister brugerne deres indk√∏bskurve.
- **Ineffektiv Resource Udnyttelse**: Nogle servere kan v√¶re overbelastede mens andre st√•r ledige, fordi brugere er "bundet" til specifikke servere.

### L√∏sninger:
1. **Eksternalis√©r Session State** (X-akse forbedring):
   - Flyt indk√∏bskurven til ekstern storage (Redis, database)
   - G√∏r alle web-servere stateless
   - Muligg√∏r √¶gte load balancing mellem identiske instanser

2. **Database/Cache Skalering**:
   - Implement√©r distributed cache (Redis Cluster)
   - Database replication for session storage

3. **Forberedelse til Y-akse**:
   - Overvej at separere indk√∏bskurv-funktionalitet som egen microservice
   - Shopping Cart API som separat komponent

---

## Opgave 2: Billetnet.dk Skalering

**Billetnet.dk s√¶lger billetter til st√∏rre arrangementer. Salget er meget stort i peaks ‚Äì som er forudsigelige. Fx 20.000 solgte billetter p√• bare 20 min., med op til 30.000 bes√∏gende. Udenfor peaks er der ikke mange bes√∏gende/salg.**

### Skalerings-strategier afh√¶ngig af bottleneck:

#### Hvis problemet er i Web-serveren (Pr√¶sentation):
**X-akse skalering:**
- Auto-scaling af web-server instanser
- Load balancer foran multiple web-servere
- CDN for statisk content (billeder, CSS, JS)
- Horizontal scaling under peaks

#### Hvis problemet er i Application-serveren (API):
**X-akse skalering:**
- Multiple API instanser med load balancing
- Container orchestration (Kubernetes) for auto-scaling

**Y-akse skalering:**
- Separer funktioner i microservices:
  - Ticket Search Service
  - Payment Processing Service
  - User Management Service
  - Event Management Service
- Queue-baseret arkitektur for ticket reservationer

#### Hvis problemet er i Databasen:
**X-akse database skalering:**
- Read replicas for s√∏gninger
- Write scaling gennem clustering

**Z-akse database skalering:**
- Partitionering efter arrangement/event
- Geografisk distribution af data

#### Kombineret Strategi (Anbefalet):
1. **X-akse**: Auto-scaling af alle lag under peaks
2. **Y-akse**: Microservices for kritiske funktioner
3. **Z-akse**: Event-baseret partitionering
4. **Queue System**: Async processing af payments
5. **Caching**: Redis for popul√¶re events og user sessions

---

## Opgave 3: Y-skalering af ConsoleSearch

**Vi skal have y-skaleret ConsoleSearch, s√• den opdeles i to komponenter: en konsol applikation som varetager interaktion med brugeren (og kun dette) og et API som indeholder s√∏gelogikken.**

### Arkitektur Design:

#### Komponent 1: Console Client
**Ansvar:** Kun brugerinteraktion
- Menu-system og bruger input
- Formatering og visning af resultater
- Configuration management (lokalt)
- HTTP client til API kommunikation

**Implementering:**
```
ConsoleSearch/
‚îú‚îÄ‚îÄ Program.cs          (Entry point)
‚îú‚îÄ‚îÄ ConsoleApp.cs       (UI logic, menu system)
‚îú‚îÄ‚îÄ ApiClient.cs        (HTTP kommunikation)
‚îú‚îÄ‚îÄ ResultFormatter.cs  (Display logic)
‚îî‚îÄ‚îÄ Config.cs          (UI preferences)
```

#### Komponent 2: Search API
**Ansvar:** S√∏gelogik og data access
- SearchLogic klassen flyttes hertil
- Database adgang
- Search algoritme og ranking
- RESTful endpoints

**Implementering:**
```
SearchAPI/
‚îú‚îÄ‚îÄ Program.cs              (Web API startup)
‚îú‚îÄ‚îÄ Controllers/
‚îÇ   ‚îî‚îÄ‚îÄ SearchController.cs (REST endpoints)
‚îú‚îÄ‚îÄ Services/
‚îÇ   ‚îî‚îÄ‚îÄ SearchService.cs    (SearchLogic klasse flyttet)
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îú‚îÄ‚îÄ SearchRequest.cs
‚îÇ   ‚îî‚îÄ‚îÄ SearchResponse.cs
‚îî‚îÄ‚îÄ Data/
    ‚îî‚îÄ‚îÄ DatabaseSqlite.cs   (Data access)
```

#### API Endpoints:
```
GET /api/search?query={query}&limit={limit}&caseSensitive={bool}&pattern={bool}
POST /api/search (for komplekse queries)
GET /api/health
```

#### Kommunikation:
- Console app sender HTTP requests til API
- JSON serialization af requests/responses
- Error handling og timeout management

### Fordele ved Y-akse split:
- **Separation of Concerns**: UI og business logic adskilt
- **Skalerbarhed**: API kan skaleres uafh√¶ngigt
- **Multi-client Support**: Andre klienter kan bruge samme API
- **Deployment Flexibility**: Komponenter kan deployes separat
- **Team Specialization**: UI og backend teams kan arbejde parallelt

---

## Opgave 4: Web-app med Blazor

**Vi skal have lavet en web-app (brug gerne Blazor) til s√∏gning, som anvender det API som blev lavet i opgave 3.**

### Blazor Server App Arkitektur:

#### Hovedkomponenter:

**1. Search Page Component**
```razor
@page "/search"
<SearchComponent />
```

**2. Search Component**
- Input felt til s√∏ge-queries
- Checkboxes for search options (case sensitive, pattern matching)
- Results display area
- Paging controls

**3. Services Integration**
```csharp
@inject ISearchService SearchService
```

#### Implementering Struktur:
```
BlazorSearchApp/
‚îú‚îÄ‚îÄ Program.cs
‚îú‚îÄ‚îÄ Pages/
‚îÇ   ‚îú‚îÄ‚îÄ Index.razor         (Landing page)
‚îÇ   ‚îî‚îÄ‚îÄ Search.razor        (Main search page)
‚îú‚îÄ‚îÄ Components/
‚îÇ   ‚îú‚îÄ‚îÄ SearchBox.razor     (Search input component)
‚îÇ   ‚îú‚îÄ‚îÄ SearchResults.razor (Results display)
‚îÇ   ‚îî‚îÄ‚îÄ SearchOptions.razor (Configuration toggles)
‚îú‚îÄ‚îÄ Services/
‚îÇ   ‚îú‚îÄ‚îÄ ISearchService.cs   (Interface)
‚îÇ   ‚îî‚îÄ‚îÄ SearchService.cs    (HTTP client til API)
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îú‚îÄ‚îÄ SearchRequest.cs    (Request DTOs)
‚îÇ   ‚îî‚îÄ‚îÄ SearchResult.cs     (Response DTOs)
‚îî‚îÄ‚îÄ wwwroot/
    ‚îú‚îÄ‚îÄ css/                (Styling)
    ‚îî‚îÄ‚îÄ js/                 (Client-side scripts)
```

#### Key Features:

**Real-time Search:**
- Blazor Server's SignalR connection for responsive UI
- Debounced input for performance
- Loading indicators under s√∏gning

**Search Options:**
- Toggle switches for case sensitivity
- Pattern matching enable/disable
- Result limit configuration
- Compact/detailed view toggle

**Results Display:**
- Paginated results
- Highlighting af search terms
- Document metadata (title, date, score)
- Responsive design for mobile/desktop

#### API Integration:
```csharp
public class SearchService : ISearchService
{
    private readonly HttpClient _httpClient;
    
    public async Task<SearchResponse> SearchAsync(SearchRequest request)
    {
        // HTTP call til Search API fra opgave 3
        var response = await _httpClient.PostAsJsonAsync("/api/search", request);
        return await response.Content.ReadFromJsonAsync<SearchResponse>();
    }
}
```

#### Fordele ved Blazor Web App:
- **Responsive UI**: Moderne web interface
- **Real-time Updates**: SignalR for live updates
- **Component Reusability**: Genbrugelige UI komponenter
- **Type Safety**: Shared DTOs mellem API og client
- **SEO Friendly**: Server-side rendering
- **Cross Platform**: Virker p√• alle browsere/enheder

### Integration Flow:
1. **User Input** ‚Üí Blazor Component
2. **Search Request** ‚Üí SearchService (HTTP client)
3. **API Call** ‚Üí Search API (fra opgave 3)
4. **Database Query** ‚Üí SearchLogic i API
5. **Results** ‚Üí JSON response tilbage gennem stack
6. **UI Update** ‚Üí Blazor re-renders results

Denne arkitektur demonstrerer fuld Y-akse skalering: UI lag (Blazor), API lag (Search API), og data lag (Database), hver med deres specifikke ansvar og skaleringsmuligheder.
</file>

<file path="indexer/App.cs">
namespace Indexer
‚ãÆ----
public class App
‚ãÆ----
public void Run(string dataset){
DatabaseSqlite db = new DatabaseSqlite(Paths.DATABASE);
Crawler crawler = new Crawler(db);
var root = new DirectoryInfo(Config.GetFolder(dataset));
DateTime start = DateTime.Now;
crawler.IndexFilesIn(root, new List<string> { ".txt"});
TimeSpan used = DateTime.Now - start;
Console.WriteLine("DONE! used " + used.TotalMilliseconds);
Console.WriteLine($"Indexed {db.DocumentCounts} documents");
Console.WriteLine($"Total word occurrences: {db.GetTotalOccurrences()}");
Console.WriteLine("How many of the most frequent words do you want to see?");
string input = Console.ReadLine();
if (int.TryParse(input, out int count) && count > 0)
‚ãÆ----
var frequentWords = db.GetMostFrequentWords(count);
Console.WriteLine($"The top {count} most frequent words are:");
‚ãÆ----
Console.WriteLine($"<{p.Item1}, {p.Item2}> - {p.Item3}");
</file>

<file path="indexer/Config.cs">
public class Config
‚ãÆ----
// Returns the folder to be indexed based on dataset size
// All .txt files in that folder (and subfolders) will be indexed
public static string GetFolder(string dataset)
‚ãÆ----
if (RuntimeInformation.IsOSPlatform(OSPlatform.Windows))
‚ãÆ----
return dataset.ToLower() switch
‚ãÆ----
_ => throw new ArgumentException($"Invalid dataset '{dataset}'. Valid options: small, medium, large")
‚ãÆ----
// macOS/Linux
</file>

<file path="indexer/Crawler.cs">
namespace Indexer
‚ãÆ----
public class Crawler
‚ãÆ----
private readonly char[] separators = " \\\n\t\"$'!,?;.:-_**+=)([]{}<>/@&%‚Ç¨#".ToCharArray();
/* Will be used to spilt text into words. So a word is a maximal sequence of
         * chars that does not contain any char from separators */
‚ãÆ----
/* Will contain all words from files during indexing - thet key is the 
         * value of the word and the value is its id in the database */
‚ãÆ----
/* Will count the number of documents indexed during indexing */
IDatabase mdatabase;
‚ãÆ----
//Return a dictionary containing all words (as the key)in the file
// [f] and the value is the number of occurrences of the key in file.
private ISet<string> ExtractWordsInFile(FileInfo f)
‚ãÆ----
var content = File.ReadAllLines(f.FullName);
‚ãÆ----
foreach (var aWord in line.Split(separators, StringSplitOptions.RemoveEmptyEntries))
‚ãÆ----
res.Add(aWord);
‚ãÆ----
private ISet<int> GetWordIdFromWords(ISet<string> src) {
‚ãÆ----
res.Add(words[p]);
‚ãÆ----
// Return a dictionary of all the words (the key) in the files contained
// in the directory [dir]. Only files with an extension in
// [extensions] is read. The value part of the return value is
// the number of occurrences of the key.
public void IndexFilesIn(DirectoryInfo dir, List<string> extensions) {
Console.WriteLine($"Crawling {dir.FullName}");
foreach (var file in dir.EnumerateFiles())
if (extensions.Contains(file.Extension))
‚ãÆ----
BEDocument newDoc = new BEDocument{
‚ãÆ----
mIdxTime = DateTime.Now.ToString(),
mCreationTime = file.CreationTime.ToString()
‚ãÆ----
mdatabase.InsertDocument(newDoc);
‚ãÆ----
if (!words.ContainsKey(aWord)) {
words.Add(aWord, words.Count + 1);
newWords.Add(aWord, words[aWord]);
‚ãÆ----
mdatabase.InsertAllWords(newWords);
mdatabase.InsertAllOcc(newDoc.mId, GetWordIdFromWords(wordsInFile));
‚ãÆ----
foreach (var d in dir.EnumerateDirectories())
</file>

<file path="indexer/DatabaseSqlite.cs">
namespace Indexer
‚ãÆ----
public class DatabaseSqlite : IDatabase
‚ãÆ----
private SqliteConnection _connection;
‚ãÆ----
var connectionStringBuilder = new SqliteConnectionStringBuilder();
‚ãÆ----
_connection = new SqliteConnection(connectionStringBuilder.ConnectionString);
_connection.Open();
‚ãÆ----
private void Execute(string sql)
‚ãÆ----
var cmd = _connection.CreateCommand();
‚ãÆ----
cmd.ExecuteNonQuery();
‚ãÆ----
public void InsertAllWords(Dictionary<string, int> res)
‚ãÆ----
using (var transaction = _connection.BeginTransaction())
‚ãÆ----
var command = _connection.CreateCommand();
‚ãÆ----
var paramName = command.CreateParameter();
‚ãÆ----
command.Parameters.Add(paramName);
var paramId = command.CreateParameter();
‚ãÆ----
command.Parameters.Add(paramId);
// Insert all entries in the res
‚ãÆ----
command.ExecuteNonQuery();
‚ãÆ----
transaction.Commit();
‚ãÆ----
public void InsertAllOcc(int docId, ISet<int> wordIds)
‚ãÆ----
var paramwordId = command.CreateParameter();
‚ãÆ----
command.Parameters.Add(paramwordId);
var paramDocId = command.CreateParameter();
‚ãÆ----
command.Parameters.Add(paramDocId);
‚ãÆ----
public void InsertWord(int id, string value)
‚ãÆ----
var insertCmd = new SqliteCommand("INSERT INTO word(id, name) VALUES(@id,@name)");
‚ãÆ----
var pName = new SqliteParameter("name", value);
insertCmd.Parameters.Add(pName);
var pCount = new SqliteParameter("id", id);
insertCmd.Parameters.Add(pCount);
insertCmd.ExecuteNonQuery();
‚ãÆ----
public void InsertDocument(BEDocument doc)
‚ãÆ----
new SqliteCommand(
‚ãÆ----
var pId = new SqliteParameter("id", doc.mId);
insertCmd.Parameters.Add(pId);
var pUrl = new SqliteParameter("url", doc.mUrl);
insertCmd.Parameters.Add(pUrl);
var pIdxTime = new SqliteParameter("idxTime", doc.mIdxTime);
insertCmd.Parameters.Add(pIdxTime);
var pCreationTime = new SqliteParameter("creationTime", doc.mCreationTime);
insertCmd.Parameters.Add(pCreationTime);
‚ãÆ----
public Dictionary<string, int> GetAllWords()
‚ãÆ----
var selectCmd = _connection.CreateCommand();
‚ãÆ----
using (var reader = selectCmd.ExecuteReader())
‚ãÆ----
while (reader.Read())
‚ãÆ----
var id = reader.GetInt32(0);
var w = reader.GetString(1);
res.Add(w, id);
‚ãÆ----
public long GetTotalOccurrences()
‚ãÆ----
return (long)selectCmd.ExecuteScalar();
‚ãÆ----
public List<(string, int, int)> GetMostFrequentWords(int limit)
‚ãÆ----
selectCmd.Parameters.AddWithValue("@limit", limit);
‚ãÆ----
res.Add((reader.GetString(0), reader.GetInt32(1), reader.GetInt32(2)));
‚ãÆ----
if (reader.Read())
‚ãÆ----
var count = reader.GetInt32(0);
</file>

<file path="indexer/IDatabase.cs">
namespace Indexer
‚ãÆ----
public interface IDatabase
‚ãÆ----
//Get all words with key as the value, and the value as the id
Dictionary<string, int> GetAllWords();
// Return the number of documents indexed in the database
‚ãÆ----
void InsertDocument(BEDocument doc);
// Insert a word in the database with id = [id] and value = [value]
void InsertWord(int id, string value);
void InsertAllWords(Dictionary<string, int> words);
void InsertAllOcc(int docId, ISet<int> wordIds);
long GetTotalOccurrences();
List<(string, int, int)> GetMostFrequentWords(int limit);
</file>

<file path="indexer/indexer.csproj">
<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net9.0</TargetFramework>
  </PropertyGroup>

  <ItemGroup>
    <None Remove="SQLitePCLRaw.core" />
    <None Remove="Microsoft.Data.Sqlite.Core" />
    <None Remove="Microsoft.Data.Sqlite" />
  </ItemGroup>
  <ItemGroup>
    <PackageReference Include="Microsoft.Data.Sqlite" Version="8.0.1" />
  </ItemGroup>
  <ItemGroup>
    <ProjectReference Include="..\Shared\Shared.csproj" />
  </ItemGroup>
</Project>
</file>

<file path="indexer/Program.cs">
namespace Indexer
‚ãÆ----
class Program
‚ãÆ----
static void Main(string[] args)
‚ãÆ----
dataset = args[0].ToLower();
‚ãÆ----
Console.WriteLine("Select dataset:");
Console.WriteLine("1. small  - 5 files (quick test)");
Console.WriteLine("2. medium - ~5,000 emails (functional + performance)");
Console.WriteLine("3. large  - ~50,000 emails (performance testing)");
Console.Write("Enter choice (small/medium/large): ");
string input = Console.ReadLine()?.ToLower().Trim();
‚ãÆ----
Console.WriteLine($"Invalid dataset '{dataset}'. Valid options: small, medium, large");
‚ãÆ----
Console.WriteLine($"Starting indexing with '{dataset}' dataset...");
new App().Run(dataset);
//new Renamer().Crawl(new DirectoryInfo(@"/Users/ole/data"));
</file>

<file path="indexer/Renamer.cs">
namespace Indexer
‚ãÆ----
public class Renamer
‚ãÆ----
public void Run()
‚ãÆ----
void RenameFile(FileInfo f)
‚ãÆ----
if  (f.FullName.EndsWith(".txt")) return;
if (f.Name.StartsWith('.')) return;
var ending = f.FullName.EndsWith(".") ? "txt" : ".txt";
File.Move(f.FullName, f.FullName + ending, true);
‚ãÆ----
public void Crawl(DirectoryInfo dir)
‚ãÆ----
Console.WriteLine("Crawling " + dir.FullName);
foreach (var file in dir.EnumerateFiles())
‚ãÆ----
foreach (var d in dir.EnumerateDirectories())
</file>

<file path="Shared/Model/BEDocument.cs">
public class BEDocument
‚ãÆ----
public String mUrl;
public String mIdxTime;
public String mCreationTime;
</file>

<file path="Shared/Paths.cs">
namespace Shared
‚ãÆ----
public class Paths
‚ãÆ----
if (RuntimeInformation.IsOSPlatform(OSPlatform.Windows))
‚ãÆ----
else if (RuntimeInformation.IsOSPlatform(OSPlatform.OSX))
‚ãÆ----
// Default to Linux/Unix style path
</file>

<file path="Shared/Shared.csproj">
<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <TargetFramework>net9.0</TargetFramework>
  </PropertyGroup>

  <ItemGroup>
    <None Remove="Model\" />
  </ItemGroup>
  <ItemGroup>
    <Folder Include="Model\" />
  </ItemGroup>
</Project>
</file>

<file path=".gitattributes">
# Auto detect text files and perform LF normalization
* text=auto
</file>

<file path=".gitignore">
# globs
Makefile.in
*.userprefs
*.usertasks
config.make
config.status
aclocal.m4
install-sh
autom4te.cache/
*.tar.gz
tarballs/
test-results/

# Mac bundle stuff
*.dmg
*.app

# content below from: https://github.com/github/gitignore/blob/main/Global/macOS.gitignore
# General
.DS_Store
.AppleDouble
.LSOverride

# Icon must end with two \r
Icon


# Thumbnails
._*

# Files that might appear in the root of a volume
.DocumentRevisions-V100
.fseventsd
.Spotlight-V100
.TemporaryItems
.Trashes
.VolumeIcon.icns
.com.apple.timemachine.donotpresent

# Directories potentially created on remote AFP share
.AppleDB
.AppleDesktop
Network Trash Folder
Temporary Items
.apdisk

# content below from: https://github.com/github/gitignore/blob/main/Global/Windows.gitignore
# Windows thumbnail cache files
Thumbs.db
ehthumbs.db
ehthumbs_vista.db

# Dump file
*.stackdump

# Folder config file
[Dd]esktop.ini

# Recycle Bin used on file shares
$RECYCLE.BIN/

# Windows Installer files
*.cab
*.msi
*.msix
*.msm
*.msp

# Windows shortcuts
*.lnk

# content below from: https://github.com/github/gitignore/blob/master/VisualStudio.gitignore
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_h.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*_wpftmp.csproj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush personal settings
.cr/personal

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder
.mfractor/

# Local History for Visual Studio
.localhistory/\"Data/searchDB.db\"
</file>

<file path=".repomixignore">
Data/
</file>

<file path="assignments.md">
# SearchEngine PoC - Opgaver og Fremgang

## Oversigt
Denne fil holder styr p√• alle opgaver til SearchEngine projektet og vores fremgang med hver opgave.

****************************************************
**MODUL 2 - S√òGEMASKINE OPGAVER**
****************************************************

## Opgave 1: Ops√¶tning og Installation
**Status**: üîÑ I gang  
**Beskrivelse**: F√• s√∏gemaskinen til at k√∏re p√• computeren

### Krav:
- [x] Koden kr√¶ver .net 9.0 - skal installeres
- [x] SQLite browser installation - se https://sqlitebrowser.org/dl/
- [x] Builde alle projekter og opdatere NuGet packages
- [x] L√¶gge dokumenter i indexering-folderen (brug seData.zip filer)
- [x] Opdatere `Shared.Paths` s√• database-stien peger rigtigt
- [x] Opdatere `Indexer.Config` s√• fil-stien til indeksering er korrekt
- [ ] K√∏re indexer programmet
- [ ] Inspicere databasen og lave stikpr√∏ver af indekserede ord
- [ ] K√∏re searchConsole og afpr√∏ve med 1 ord, 2 ord og flere ord

### üîß S√•dan K√∏rer Du Systemet:

**1. Build projektet:**
```bash
cd "C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main"
dotnet build
```

**2. K√∏r indexer (opretter database og indekserer filer):**
```bash
cd indexer
dotnet run
```

**3. K√∏r s√∏gemaskinen:**
```bash
cd ConsoleSearch
dotnet run
```

**4. Inspicer databasen (SQLite Browser):**
- √Öbn SQLite Browser (DB Browser for SQLite)
- File ‚Üí Open Database
- V√¶lg: `C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data\searchDB.db`
- Se p√• tabellerne: `document`, `word`, `occ` (occurrence)
- Lav stikpr√∏ver - check at dokumenter og ord er indekseret korrekt

**5. Test s√∏gninger:**
- Skriv et ord og tryk Enter
- Skriv flere ord adskilt af mellemrum
- Skriv `q` for at afslutte

### Nuv√¶rende Konfiguration (skal √¶ndres):
```csharp
// Shared\Paths.cs
public static string DATABASE = @"/Users/oleeriksen/Data/searchDBmedium.db";

// indexer\Config.cs  
public static string FOLDER = @"/Users/oleeriksen/Data/seData/medium";
```

### ‚úÖ Hvad Vi Har Gjort:
- [x] Analyseret hele projektet og forst√•et arkitekturen
- [x] Identificeret konfigurations-filer der skal opdateres
- [x] Opdateret `Shared\Paths.cs` med Windows database sti
- [x] Opdateret `indexer\Config.cs` med Windows folder sti
- [x] Buildet projekterne succesfuldt (0 warnings, 0 errors)
- [x] K√∏rt indexer og oprettet database (13MB, 3034 dokumenter, 31079 forskellige ord)
- [ ] Testet s√∏gemaskinen med forskellige s√∏gninger

### üìÅ Opdaterede Windows Stier:
```csharp
// Shared\Paths.cs:
DATABASE = @"C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data\searchDB.db";

// indexer\Config.cs:
FOLDER = @"C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data\seData copy\medium";
```

### üéâ Indexer Resultater:
```
DONE! used 111991,158 ms (1 minut 52 sekunder)
Indexed 3034 documents
Number of different words: 31079
Database size: 13 MB
```

**De f√∏rste 10 ord i indekset:**
1. Message (ID: 1)
2. ID (ID: 2) 
3. 29790972 (ID: 3)
4. 1075855665306 (ID: 4)
5. JavaMail (ID: 5)
6. evans (ID: 6)
7. thyme (ID: 7)
8. Date (ID: 8)
9. Wed (ID: 9)
10. 13 (ID: 10)

### üóÉÔ∏è Database Struktur (3 tabeller):

**1. `document` tabel:**
- `id` (INTEGER PRIMARY KEY) - Unikt dokument ID
- `url` (TEXT) - Fil sti til dokumentet
- `idxTime` (TEXT) - Tidspunkt for indeksering
- `creationTime` (TEXT) - Dokumentets oprettelsestidspunkt

**2. `word` tabel:**
- `id` (INTEGER PRIMARY KEY) - Unikt ord ID  
- `name` (VARCHAR(50)) - Selve ordet

**3. `occ` (occurrence) tabel:**
- `wordId` (INTEGER) - Reference til word.id
- `docId` (INTEGER) - Reference til document.id
- Foreign keys + index p√• wordId for hurtig s√∏gning

**S√•dan inspiceres databasen:**
1. √Öbn DB Browser for SQLite
2. Open Database ‚Üí v√¶lg `searchDB.db`
3. Klik p√• "Browse Data" tab
4. V√¶lg tabel for at se indhold:
   - `document`: Se alle 3034 indekserede dokumenter
   - `word`: Se alle 31079 forskellige ord 
   - `occ`: Se ord-dokument relationer (mange-til-mange)

============================================

## Opgave 2: Forbedret Statistik
**Status**: ‚è≥ Afventer opgave 1  
**Beskrivelse**: √Ündre indexer output til at vise ord-hyppighed

### Krav:
- Informere om hvor mange ord forekomster der er indekseret
- Sp√∏rge brugeren hvor mange ord de vil se
- Vise ord rangeret efter hyppighed (hyppigste f√∏rst)
- Output format: `<Message, 1> - 48342` (ord, id, hyppighed)

### Hvad Vi Har Gjort:
- [ ] Endnu ikke p√•begyndt

---

## Opgave 3: Case Sensitivity Kontrol  
**Status**: ‚è≥ Afventer opgave 1  
**Beskrivelse**: G√∏r s√∏gning case-sensitiv kontrollerbar

### Krav:
- Bruger kan skrive `/casesensitive=on` eller `/casesensitive=off`
- Alternativ: Config klasse med `CaseSensitive` boolean attribut

### Hvad Vi Har Gjort:
- [ ] Endnu ikke p√•begyndt

---

## Opgave 4: Timestamp Visning
**Status**: ‚è≥ Afventer opgave 1  
**Beskrivelse**: Brugeren kan v√¶lge om tidsstempel skal vises

### Krav:
- Kommandoer som `/timestamp=on` eller `/timestamp=off`
- Alternativ: `ViewTimeStamps` boolean i Config klasse

### Hvad Vi Har Gjort:
- [ ] Endnu ikke p√•begyndt

---

## Opgave 5: Konfigurerbar Resultat Gr√¶nse
**Status**: ‚è≥ Afventer opgave 1  
**Beskrivelse**: √Ündre fra faste 10 resultater til bruger-valgt antal

### Krav:
- √Ündre fra 10 til 20 som standard
- Kommandoer som `/results=15` eller `/results=all`
- Alternativ: `int?` attribut i Config (null = alle resultater)

### Hvad Vi Har Gjort:
- [ ] Endnu ikke p√•begyndt

---

## Opgave 6: M√∏nster-s√∏gning (Avanceret)
**Status**: ‚è≥ Afventer opgave 1  
**Beskrivelse**: Wildcard/regul√¶re udtryk s√∏gning

### Krav:
- `?` = et vilk√•rligt tegn
- `*` = vilk√•rligt antal tegn (ogs√• ingen)
- Eksempel: `BJ????7` matcher 7-tegns ord startende med BJ, sluttende med 7
- Vis matchende termer i resultatet
- F√∏rste version: kun et m√∏nster ad gangen (ikke kombineret med normale s√∏geord)

### Eksempel Output:
```
enter search terms - q for quit
BJ????7
Pattern Search
1: /path/to/document.txt -- contains 3 matching terms:
BJ12347, BJERGE7, BJ73857
```

### Hvad Vi Har Gjort:
- [ ] Endnu ikke p√•begyndt

---

## Noter og Overvejelser

### Skole Projekt Begr√¶nsninger:
- Vi m√• ikke n√∏dvendigvis redigere alle filer i projektet
- Kan v√¶re begr√¶nsninger i at oprette nye filer
- Simplicitet er med vilje - det er et uddannelsesprojekt

### Test Data:
- Small dataset: 13 emails (funktionel test)
- Medium dataset: ~5000 emails (funktionel + performance)  
- Large dataset: ~50000 emails (performance test)

### Vigtige Filer at Forst√•:
- `indexer\App.cs` - Indeksering workflow
- `indexer\Crawler.cs` - Tekst udtr√¶kning og ord parsing
- `ConsoleSearch\App.cs` - S√∏ge interface
- `ConsoleSearch\SearchLogic.cs` - S√∏ge algoritme
- `Shared\Paths.cs` - Konfiguration der skal opdateres
</file>

<file path="Claude.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

# SearchEngine PoC - Project Analysis

## Project Overview
This is a **Proof of Concept (PoC) Search Engine** for IT-Architecture semester 6 at Erhvervsakademiet Aarhus. The system is designed as an internal document search solution for organizations with 50+ employees.

**Important Note**: This is a school project with restrictions on file modifications and creation. Not all files may be editable, and the simplicity is intentional for educational purposes.

## Architecture & Components

### Solution Structure
- **`indexer`** - Console application that crawls and indexes documents
- **`ConsoleSearch`** - Console application providing search functionality  
- **`Shared`** - Class library containing common models and configuration

### Technology Stack
- **.NET 9.0** C# console applications
- **SQLite database** for inverted index storage
- **Microsoft.Data.Sqlite** NuGet package (version 8.0.1)

### Database Schema (Inverted Index)
SQLite database with three main tables:
```sql
Document: docId, title, link, date
Word: termId, value  
Occurrence: docId, termId (many-to-many relationship)
```

## Core Components Analysis

### 1. Indexer (`indexer` project)
**Entry Point**: `Program.cs` ‚Üí `App.cs`
- **Configuration**: `Config.cs` - defines folder to index
- **Crawler**: `Crawler.cs` - main indexing logic
- **Database**: `DatabaseSqlite.cs` - handles database operations

**Key Implementation Details**:
- Only indexes `.txt` files recursively
- Word extraction separators: `" \\\n\t\"$'!,?;.:-_**+=)([]{}<>/@&%‚Ç¨#"`
- Creates inverted index: word ‚Üí documents containing it
- Platform-specific database paths via `RuntimeInformation`

**Indexing Workflow**:
1. Prompts user to select dataset size (small/medium/large)
2. Recursively crawls configured directory for `.txt` files
3. Extracts and normalizes words using defined separators
4. Builds inverted index in SQLite database
5. Outputs comprehensive statistics including:
   - Total documents indexed
   - Total word occurrences
   - Top N most frequent words (user-configurable)

### 2. Search Engine (`ConsoleSearch` project)
**Entry Point**: `Program.cs` ‚Üí `App.cs`
- **Search Logic**: `SearchLogic.cs` - implements search algorithm
- **Database**: `DatabaseSqlite.cs` - read-only database access
- **Models**: `SearchResult.cs`, `DocumentHit.cs`
- **Config**: `Config.cs` - feature toggles (case sensitivity, timestamps, result limits, pattern search, compact view)

**Search Workflow**:
1. Displays interactive menu with feature toggles
2. Accepts queries (normal search or pattern matching)
3. Maps query terms to word IDs in database
4. Finds intersecting documents using inverted index
5. Calculates relevance scores and ranks results
6. Returns configurable number of results (default 20) with metadata

**Scoring Algorithm**: 
```
score = (number_of_matching_terms / total_query_terms)
```

### 3. Shared Library (`Shared` project)
- **`BEDocument.cs`**: Document business entity model
- **`Paths.cs`**: Cross-platform database path configuration (auto-detects Windows/macOS/Linux)
- **`IDatabase.cs`**: Database interface (used by both indexer and search)

## Test Data Structure

### Enron Email Dataset
Located in: `C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data\seData copy\`

**Three Dataset Sizes**:
- **small**: 13 emails in 1 folder (functional testing)
- **medium**: ~5,000 emails in ~20 mailboxes (functional + performance)
- **large**: ~50,000 emails for 15 users (performance testing)

**Data Format**: Email files with headers and content
```
Message-ID: <...>
Date: Wed, 13 Dec 2000 18:41:00 -0800 (PST)
From: sender@domain.com
To: recipient@domain.com
Subject: Email Subject
...email content...
```

## Requirements (from Danish Documentation)

### Business Context
- Organization with 50+ employees producing documents
- Documents in multiple formats (docx, pdf, rtf, txt, csv, pptx, xlsx)
- Multi-terabyte collections across file servers
- Need for "instant content search"
- Domain-specific customization capability

### Technical Requirements
- **100% recall**: All documents containing any search terms must appear
- **Ranking**: Results ordered by descending score
- **Scoring**: Percentage of query terms found in document (0.0 to 1.0)
- **Future features**: Synonym dictionary for domain-specific terms

### User Interface Requirements
- Google-like search interface
- Results showing: title, link, date, score, snippet
- Can be web application (intranet) or desktop application

## Essential Commands

### Build Solution
```bash
dotnet build SearchEngine.sln
```

### Run Projects
```bash
# Run indexer with dataset selection (crawls and indexes documents)
cd indexer
dotnet run small     # Index small dataset (13 emails)
dotnet run medium    # Index medium dataset (~5,000 emails) 
dotnet run large     # Index large dataset (~50,000 emails)
# Alternative: dotnet run (will prompt for dataset selection)

# Run search console (interactive search)
cd ConsoleSearch
dotnet run
```

### Restore Packages
```bash
dotnet restore
```

### Configuration Setup
**No manual configuration needed!** The system automatically detects your platform (Windows/macOS/Linux) and uses appropriate paths:
- `Shared/Paths.cs` - Database path (auto-detects platform)
- `indexer/Config.cs` - Dataset folders (auto-detects platform)

### Database Inspection
Use SQLite browser to inspect `Data/searchDB.db` after indexing.

### Manual Testing Approach
**No formal test framework** - this project uses manual testing with realistic datasets:
- **Small dataset**: 13 emails for functional verification
- **Medium dataset**: ~5,000 emails for functional + basic performance testing  
- **Large dataset**: ~50,000 emails for performance testing
- Test search functionality with 1-word, 2-word, and multi-word queries
- Verify interactive menu options and configuration toggles

### Available Commands in Search Console
- **Menu Options**: `1`, `2`, `3`, `4`, `5` - Toggle various settings
- **Help**: `?` - Display comprehensive help with current settings and examples
- **Quit**: `q` - Exit the application
- **Slash Commands**: 
  - `/casesensitive=on|off` - Toggle case sensitivity
  - `/timestamp=on|off` - Toggle timestamp display
  - `/results=NUMBER|all` - Set result limit
  - `/pattern=on|off` - Toggle pattern search mode
  - `/compact=on|off` - Toggle compact view display

## Cross-Platform Configuration
‚úÖ **Automatic platform detection implemented** via `RuntimeInformation.IsOSPlatform()`:
- **Database paths**: Automatically selected based on detected OS
- **Dataset folders**: Platform-specific paths configured in `indexer/Config.cs`
- **No manual path changes needed** when switching between systems
- See `Shared/Paths.cs` and `indexer/Config.cs` for implementation details

## Available Assignments (from Danish docs)

### Assignment 1: Setup
- Install .NET 9.0
- Install SQLite browser
- Build all projects, update NuGet packages
- Update paths in `Shared.Paths` and `Indexer.Config`
- Run indexer, inspect database
- Test search console with 1, 2, and multiple word queries

### Assignment 2: Enhanced Statistics ‚úÖ **COMPLETED**
- ‚úÖ Shows total word occurrences indexed
- ‚úÖ Prompts user for number of most frequent words to display
- ‚úÖ Displays words ranked by frequency (most frequent first)
- ‚úÖ Format: `<word, id> - frequency`

### Assignment 3: Case Sensitivity Control ‚úÖ **COMPLETED**
- ‚úÖ Interactive menu option "1" to toggle case sensitivity
- ‚úÖ `Config.CaseSensitive` boolean controls behavior
- ‚úÖ Default: case-sensitive search enabled

### Assignment 4: Timestamp Display Control ‚úÖ **COMPLETED**
- ‚úÖ Interactive menu option "2" to toggle timestamp display
- ‚úÖ `Config.ViewTimeStamps` boolean controls display
- ‚úÖ Default: timestamps shown in results

### Assignment 5: Result Limit Configuration ‚úÖ **COMPLETED**
- ‚úÖ Interactive menu option "3" to configure result limits
- ‚úÖ Supports specific numbers (e.g., 15) or "all" for unlimited
- ‚úÖ `Config.ResultLimit` int? property (default: 20)
- ‚úÖ Changed from fixed 10 to configurable system

### Assignment 6: Pattern Matching ‚úÖ **COMPLETED**
- ‚úÖ Interactive menu option "4" to enable pattern search mode
- ‚úÖ Supports `?` (single char) and `*` (multiple chars) wildcards
- ‚úÖ `Config.PatternSearch` boolean toggle
- ‚úÖ Shows matching terms in results

### Additional Enhancements ‚úÖ **IMPLEMENTED**
- ‚úÖ **Compact View** - Option "5" for clean, single-line result display
- ‚úÖ **Help System** - Type `?` for comprehensive contextual help
- ‚úÖ **Enhanced Commands** - Additional slash commands (`/compact=on|off`)

## Current State Assessment

### ‚úÖ Working Components
- Basic indexing and search functionality
- Clean three-project architecture
- Realistic test data (Enron dataset)
- Proper inverted index implementation
- Score-based ranking
- Cross-platform support (Windows/macOS/Linux)
- Enhanced statistics with word frequency
- Case sensitivity control (`Config.CaseSensitive`)
- Configurable result limits (`Config.ResultLimit`)
- Timestamp display control (`Config.ViewTimeStamps`)
- Pattern matching with wildcards (`Config.PatternSearch`)
- Compact view for clean result display (`Config.CompactView`)
- Interactive menu system for feature toggles
- Comprehensive help system with contextual examples (accessible via `?`)

### ‚ö†Ô∏è Areas for Future Enhancement
- No snippets in search results yet
- No synonym dictionary support
- Could expand wildcard patterns beyond single/multiple character matching

### üö´ School Project Constraints
- Limited ability to edit certain files
- Cannot create arbitrary new files
- Simplicity is intentional for educational purposes
- Focus on extending existing functionality vs. major architectural changes

## Assignment Progress Tracking

**üìã See `assignments.md` for detailed progress tracking in Danish**

The `assignments.md` file contains:
- All 6 assignments from the Danish documentation
- Current status and progress for each assignment
- Detailed steps and requirements
- What has been completed vs. what remains

**Note**: All core assignments (1-6) have been implemented, plus additional user experience enhancements. The system now includes enhanced statistics, configurable search options, pattern matching capabilities, compact view display, and a comprehensive help system. See `assignments.md` for detailed implementation status.

## Academic Course Context

### Current Module: IT-Architecture Semester 6
- **Module 2**: Search Engine PoC (completed assignments 1-6)
- **Module 3**: AKF Scale Cube architecture patterns
- Course materials located in `Documents/Modul X - Agenda/` folders
- Assignment responses and analysis files created as course progresses

### Course Materials Structure
- **Reading materials**: `L√¶selektier.txt` and `L√¶selektier.pdf` files
- **Assignment responses**: `Opgaver_ModulX.txt` files with Danish answers
- **Database guides**: Technical documentation for SQLite inspection

## Getting Started Checklist
1. Verify .NET 9.0 installation
2. ‚úÖ Cross-platform paths already configured automatically
3. Install SQLite browser for database inspection
4. Build solution: `dotnet build SearchEngine.sln`
5. Run indexer with dataset selection: `cd indexer && dotnet run medium`
6. Test search functionality: `cd ConsoleSearch && dotnet run`
7. Explore interactive menu features (case sensitivity, timestamps, result limits, pattern search, compact view)
8. Try the help system by typing `?` to see all available options and examples

## Architecture Notes

### Key Files
- `indexer/App.cs` - Indexing main workflow at indexer:32
- `indexer/Crawler.cs` - Text extraction and word parsing logic
- `ConsoleSearch/App.cs` - Interactive search console interface
- `ConsoleSearch/SearchLogic.cs` - Search algorithm and ranking at ConsoleSearch:45
- `Shared/Paths.cs` - Cross-platform database path configuration at Shared:7

### Search Algorithm Details
The system implements a basic TF (term frequency) scoring model where each document's relevance score is calculated as the percentage of query terms found within it. The inverted index allows efficient lookup of documents containing specific terms, with results ranked by descending relevance score and limited by the configurable result limit (default: 20).

### Configuration Dependencies
Both applications use automatic cross-platform configuration:
- `Shared/Paths.cs` uses `RuntimeInformation` to detect platform and select appropriate database paths
- `indexer/Config.cs` uses `RuntimeInformation` to select appropriate dataset folder paths
- `ConsoleSearch/Config.cs` provides feature toggles for search behavior:
  - `CaseSensitive` - Case-sensitive search matching
  - `ViewTimeStamps` - Display document indexing timestamps  
  - `ResultLimit` - Maximum results to display (int? - null = unlimited)
  - `PatternSearch` - Wildcard pattern matching mode
  - `CompactView` - Clean single-line result display format

### Interactive Features
The search console (`ConsoleSearch/App.cs`) provides an interactive menu system allowing users to toggle:
1. **Case Sensitivity** - Enable/disable case-sensitive search
2. **Timestamp Display** - Show/hide document timestamps in results
3. **Result Limits** - Configure number of results (default 20, or "all")
4. **Pattern Search** - Enable wildcard matching with `?` (single char) and `*` (multiple chars)
5. **Compact View** - Clean display format removing long file paths, showing results as single lines

**Enhanced Help System**: Users can type `?` (or option `6`) to access comprehensive contextual help showing current settings, examples, and available commands.
</file>

<file path="README.txt">
Version 1: 04-08-2025

This codebase is s PoC seachengine that consist of two programs and a class library.

The two programs are the indexer (also called a crawler) and a search program. Both
are simple console programs.

The indexer will crawl a folder (in depth) and create a reverse index
in a database. It will only index text files with .txt as extension.

The search program (see the ConsoleSearch project) offers a query-based search
in the reverse index.

The class library Shared contains classes that are used by the indexer
and the ConsoleSearch. It contains:

- Paths containing a static path the database (used by both the indexer (write-only), and
the search program (read-only).
- BEDocument (BE for Business Entity) - a class representing a document.
</file>

<file path="SearchEngine.sln">
Microsoft Visual Studio Solution File, Format Version 12.00
# Visual Studio Version 16
VisualStudioVersion = 16.0.810.10
MinimumVisualStudioVersion = 10.0.40219.1
Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "indexer", "indexer\indexer.csproj", "{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}"
EndProject
Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "ConsoleSearch", "ConsoleSearch\ConsoleSearch.csproj", "{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}"
EndProject
Project("{2150E333-8FDC-42A3-9474-1A3956D46DE8}") = "Solution Items", "Solution Items", "{72D568E9-23A6-46FD-97F0-7411FC0B2C8D}"
	ProjectSection(SolutionItems) = preProject
		README.txt = README.txt
	EndProjectSection
EndProject
Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "Shared", "Shared\Shared.csproj", "{3551940F-5CAC-4EB7-8514-629705C73335}"
EndProject
Global
	GlobalSection(SolutionConfigurationPlatforms) = preSolution
		Debug|Any CPU = Debug|Any CPU
		Release|Any CPU = Release|Any CPU
	EndGlobalSection
	GlobalSection(ProjectConfigurationPlatforms) = postSolution
		{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}.Debug|Any CPU.ActiveCfg = Debug|Any CPU
		{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}.Debug|Any CPU.Build.0 = Debug|Any CPU
		{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}.Release|Any CPU.ActiveCfg = Release|Any CPU
		{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}.Release|Any CPU.Build.0 = Release|Any CPU
		{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}.Debug|Any CPU.ActiveCfg = Debug|Any CPU
		{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}.Debug|Any CPU.Build.0 = Debug|Any CPU
		{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}.Release|Any CPU.ActiveCfg = Release|Any CPU
		{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}.Release|Any CPU.Build.0 = Release|Any CPU
		{3551940F-5CAC-4EB7-8514-629705C73335}.Debug|Any CPU.ActiveCfg = Debug|Any CPU
		{3551940F-5CAC-4EB7-8514-629705C73335}.Debug|Any CPU.Build.0 = Debug|Any CPU
		{3551940F-5CAC-4EB7-8514-629705C73335}.Release|Any CPU.ActiveCfg = Release|Any CPU
		{3551940F-5CAC-4EB7-8514-629705C73335}.Release|Any CPU.Build.0 = Release|Any CPU
	EndGlobalSection
	GlobalSection(SolutionProperties) = preSolution
		HideSolutionNode = FALSE
	EndGlobalSection
	GlobalSection(ExtensibilityGlobals) = postSolution
		SolutionGuid = {0EBC1CCA-0BFD-4D41-AE7E-1157EA73535D}
	EndGlobalSection
EndGlobal
</file>

</files>
