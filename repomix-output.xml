This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where empty lines have been removed, content has been compressed (code blocks are separated by ⋮---- delimiter).

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Empty lines have been removed from all files
- Content has been compressed - code blocks are separated by ⋮---- delimiter
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.claude/
  settings.local.json
ConsoleSearch/
  App.cs
  Config.cs
  ConsoleSearch.csproj
  DatabaseSqlite.cs
  DocumentHit.cs
  IDatabase.cs
  PatternDocumentHit.cs
  PatternSearchResult.cs
  Program.cs
  SearchLogic.cs
  SearchResult.cs
Documents/
  Modul 2 - Agenda/
    Assignemnts day 1.txt
    Database_Inspection_Guide.md
    Intro to Case.txt
    Test Data explanation.txt
  Modul 3 - Agenda/
    Læselektier.txt
    Opgaver_Modul3.txt
indexer/
  App.cs
  Config.cs
  Crawler.cs
  DatabaseSqlite.cs
  IDatabase.cs
  indexer.csproj
  Program.cs
  Renamer.cs
Shared/
  Model/
    BEDocument.cs
  Paths.cs
  Shared.csproj
.gitattributes
.gitignore
.repomixignore
assignments.md
Claude.md
README.txt
SearchEngine.sln
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".claude/settings.local.json">
{
  "permissions": {
    "allow": [
      "Bash(dotnet --version)",
      "Bash(dotnet build:*)"
    ],
    "deny": [],
    "ask": []
  }
}
</file>

<file path="ConsoleSearch/App.cs">
namespace ConsoleSearch
⋮----
public class App
⋮----
public void Run()
⋮----
SearchLogic mSearchLogic = new SearchLogic(new DatabaseSqlite());
Console.WriteLine("Console Search");
⋮----
string input = Console.ReadLine();
if (string.IsNullOrEmpty(input)) continue;
// Quit
if (input.Equals("q", StringComparison.OrdinalIgnoreCase)) break;
// Help (accept both '?' and optional '6')
⋮----
Console.Write("Enter new result limit (e.g., 15, or 'all'): ");
string limitInput = Console.ReadLine();
if (!string.IsNullOrEmpty(limitInput))
⋮----
if (limitInput.Equals("all", StringComparison.OrdinalIgnoreCase))
⋮----
else if (int.TryParse(limitInput, out int limit) && limit > 0)
⋮----
Console.WriteLine("Invalid input. Limit unchanged.");
⋮----
var patternResult = mSearchLogic.PatternSearch(input);
Console.WriteLine("Pattern Search Results:");
⋮----
string fileName = System.IO.Path.GetFileName(hit.Document.mUrl);
⋮----
Console.WriteLine($"{patternIdx}. {fileName} ({hit.MatchingWords.Count} {matchText}): {string.Join(", ", hit.MatchingWords)}");
⋮----
Console.WriteLine($"{patternIdx}: {hit.Document.mUrl} -- contains {hit.MatchingWords.Count} matching terms:");
Console.WriteLine($"    {string.Join(", ", hit.MatchingWords)}");
⋮----
Console.WriteLine($"Found {patternResult.Hits.Count} documents.");
⋮----
else if (input.StartsWith("/"))
⋮----
var query = input.Split(" ", StringSplitOptions.RemoveEmptyEntries);
var searchResult = mSearchLogic.Search(query);
⋮----
Console.WriteLine($"Ignored: {string.Join(',', searchResult.Ignored)}");
⋮----
string fileName = System.IO.Path.GetFileName(doc.Document.mUrl);
⋮----
// Get the matching terms (all query terms minus missing ones)
var allTerms = query.ToList();
var matchingTerms = allTerms.Except(doc.Missing).ToArray();
Console.WriteLine($"{searchIdx}. {fileName} ({doc.NoOfHits} {hitText}): {string.Join(", ", matchingTerms)}");
⋮----
Console.WriteLine($"{searchIdx} : {doc.Document.mUrl} -- contains {doc.NoOfHits} search terms");
⋮----
Console.WriteLine("    Index time: " + doc.Document.mIdxTime);
⋮----
Console.WriteLine($"    Missing: {ArrayAsString(doc.Missing.ToArray())}");
⋮----
Console.WriteLine("Documents: " + searchResult.Hits + ". Time: " + searchResult.TimeUsed.TotalMilliseconds);
⋮----
private void DisplayMenu()
⋮----
Console.WriteLine("\n----------------------------------------------------------------");
⋮----
string limitStatus = Config.ResultLimit.HasValue ? Config.ResultLimit.Value.ToString() : "ALL";
⋮----
Console.WriteLine($"OPTIONS: [1] Case Sensitive: {caseStatus}   [2] Timestamps: {timeStatus}   [3] Result Limit: {limitStatus}   [4] Pattern Search: {patternStatus}   [5] Compact View: {compactStatus}");
Console.WriteLine("Enter a search query, a number to toggle/change, '?' for help, or 'q' to quit.");
Console.Write("> ");
⋮----
private void DisplayHelp()
⋮----
Console.WriteLine();
Console.WriteLine("=== SEARCH ENGINE HELP ===");
⋮----
Console.WriteLine("CURRENT SETTINGS:");
// Case Sensitive
Console.WriteLine($"Case Sensitive: {(Config.CaseSensitive ? "ON" : "OFF")} - When ON, letter casing must match exactly.");
Console.WriteLine($"  Example: Query 'Hello' {(Config.CaseSensitive ? "will NOT match 'hello'" : "will match 'hello'")} in documents.");
Console.WriteLine("  Toggle via: [1] or /casesensitive=on|off");
⋮----
// Timestamps
Console.WriteLine($"Timestamps: {(Config.ViewTimeStamps ? "ON" : "OFF")} - Shows document indexing time under each result.");
Console.WriteLine("  Example line: 'Index time: 2025-01-15 14:30:22'");
Console.WriteLine("  Toggle via: [2] or /timestamp=on|off");
⋮----
// Result Limit
⋮----
Console.WriteLine($"Result Limit: {limitStatus} - Maximum number of results displayed (ALL = no limit).");
Console.WriteLine("  Example: If set to 20, only top 20 matches are shown.");
Console.WriteLine("  Change via: [3] or /results=NUMBER|all");
⋮----
// Pattern Search
Console.WriteLine($"Pattern Search: {(Config.PatternSearch ? "ON" : "OFF")} - Enables wildcard matching using ? and *.");
Console.WriteLine("  ? matches exactly one character. * matches zero or more characters.");
Console.WriteLine("  Example pattern: en*gy?  -> matches 'energy1', 'enggy2', 'enxxxgyA'");
Console.WriteLine("  Toggle via: [4]" + " or /pattern=on|off");
⋮----
// Compact View
Console.WriteLine($"Compact View: {(Config.CompactView ? "ON" : "OFF")} - Shows clean, simplified search results.");
Console.WriteLine("  Removes long file paths and displays results on single lines.");
Console.WriteLine("  This will also hide timestamps regardless of that setting.");
Console.WriteLine("  Example: '1. file.txt (3 matches): energy, power, grid' instead of full paths");
Console.WriteLine("  Toggle via: [5] or /compact=on|off");
⋮----
// Other commands
Console.WriteLine("OTHER:");
Console.WriteLine("- '?' shows this help.");
Console.WriteLine("- 'q' quits the application.");
⋮----
Console.WriteLine("Press any key to return to the menu...");
Console.ReadKey(true);
⋮----
private void HandleCommand(string input)
⋮----
var parts = input.Split('=');
var command = parts[0].ToLower();
var value = parts.Length > 1 ? parts[1].ToLower() : "";
⋮----
Console.WriteLine($"Case sensitivity set to {Config.CaseSensitive}");
⋮----
Console.WriteLine($"Timestamp display set to {Config.ViewTimeStamps}");
⋮----
else if (int.TryParse(value, out int limit) && limit > 0) Config.ResultLimit = limit;
⋮----
Console.WriteLine($"Result limit set to {limitStatus}");
⋮----
Console.WriteLine($"Pattern search set to {(Config.PatternSearch ? "ON" : "OFF")}");
⋮----
Console.WriteLine($"Compact view set to {(Config.CompactView ? "ON" : "OFF")}");
⋮----
Console.WriteLine($"Unknown command: {command}");
⋮----
string ArrayAsString(string[] s)
⋮----
return s.Length == 0 ? "[]" : $"[{String.Join(',', s)}]";
</file>

<file path="ConsoleSearch/Config.cs">
namespace ConsoleSearch
⋮----
public static class Config
</file>

<file path="ConsoleSearch/ConsoleSearch.csproj">
<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net9.0</TargetFramework>
  </PropertyGroup>

  <ItemGroup>
    <None Remove="SQLitePCLRaw.core" />
    <None Remove="Microsoft.Data.Sqlite" />
  </ItemGroup>
  <ItemGroup>
    <PackageReference Include="Microsoft.Data.Sqlite" Version="8.0.1" />
  </ItemGroup>
  <ItemGroup>
    <ProjectReference Include="..\Shared\Shared.csproj" />
  </ItemGroup>
</Project>
</file>

<file path="ConsoleSearch/DatabaseSqlite.cs">
namespace ConsoleSearch
⋮----
public class DatabaseSqlite : IDatabase
⋮----
private SqliteConnection _connection;
⋮----
var connectionStringBuilder = new SqliteConnectionStringBuilder();
⋮----
_connection = new SqliteConnection(connectionStringBuilder.ConnectionString);
_connection.Open();
⋮----
private void Execute(string sql)
⋮----
var cmd = _connection.CreateCommand();
⋮----
cmd.ExecuteNonQuery();
⋮----
// key is the id of the document, the value is number of search words in the document
public List<KeyValuePair<int, int>> GetDocuments(List<int> wordIds)
⋮----
/* Example sql statement looking for doc id's that
               contain words with id 2 and 3
               SELECT docId, COUNT(wordId) as count
                 FROM Occ
                WHERE wordId in (2,3)
             GROUP BY docId
             ORDER BY COUNT(wordId) DESC 
             */
⋮----
var selectCmd = _connection.CreateCommand();
⋮----
using (var reader = selectCmd.ExecuteReader())
⋮----
while (reader.Read())
⋮----
var docId = reader.GetInt32(0);
var count = reader.GetInt32(1);
res.Add(new KeyValuePair<int, int>(docId, count));
⋮----
private string AsString(List<int> x) => $"({string.Join(',', x)})";
private Dictionary<string, int> GetAllWords()
⋮----
var id = reader.GetInt32(0);
var w = reader.GetString(1);
res.Add(w, id);
⋮----
public List<BEDocument> GetDocDetails(List<int> docIds)
⋮----
var url = reader.GetString(1);
var idxTime = reader.GetString(2);
var creationTime = reader.GetString(3);
res.Add(new BEDocument { mId = id, mUrl = url, mIdxTime = idxTime, mCreationTime = creationTime });
⋮----
/* Return a list of id's for words; all them among wordIds, but not present in the document
         */
public List<int> getMissing(int docId, List<int> wordIds)
⋮----
var wordId = reader.GetInt32(0);
present.Add(wordId);
⋮----
result.Remove(w);
⋮----
public List<string> WordsFromIds(List<int> wordIds)
⋮----
var wordId = reader.GetString(0);
result.Add(wordId);
⋮----
public List<int> GetWordIds(string[] query, out List<string> outIgnored)
⋮----
var command = _connection.CreateCommand();
⋮----
command.Parameters.AddWithValue("@name", aWord);
⋮----
using (var reader = command.ExecuteReader())
⋮----
if (reader.Read())
⋮----
res.Add(reader.GetInt32(0));
⋮----
ignored.Add(aWord);
⋮----
public List<string> GetWordsMatchingPattern(string pattern)
⋮----
// GLOB is case-sensitive and uses * and ? as wildcards, which matches the user's input format.
⋮----
command.Parameters.AddWithValue("@pattern", pattern); // Use the original pattern
⋮----
// LIKE is case-insensitive by default. It uses % and _.
var sqlPattern = pattern.Replace('?', '_').Replace('*', '%');
⋮----
command.Parameters.AddWithValue("@pattern", sqlPattern);
⋮----
res.Add(reader.GetString(0));
⋮----
public Dictionary<int, List<string>> GetDocsWithMatchingWords(List<string> matchingWords)
⋮----
// Step 1: Get word IDs and create lookup maps
⋮----
var wordParams = string.Join(",", matchingWords.Select((_, i) => $"@p{i}"));
⋮----
command.Parameters.AddWithValue($"@p{i}", matchingWords[i]);
⋮----
int id = reader.GetInt32(0);
string name = reader.GetString(1);
⋮----
// Step 2: Get all occurrences for the found word IDs
var idParams = string.Join(",", wordIdToName.Keys.Select((_, i) => $"@id{i}"));
var occCommand = _connection.CreateCommand();
⋮----
occCommand.Parameters.AddWithValue($"@id{j++}", id);
⋮----
// Step 3: Map docIds to the list of matching words they contain
using (var reader = occCommand.ExecuteReader())
⋮----
int docId = reader.GetInt32(0);
int wordId = reader.GetInt32(1);
if (!result.ContainsKey(docId))
⋮----
result[docId].Add(wordIdToName[wordId]);
</file>

<file path="ConsoleSearch/DocumentHit.cs">
namespace ConsoleSearch
⋮----
public class DocumentHit
</file>

<file path="ConsoleSearch/IDatabase.cs">
namespace ConsoleSearch
⋮----
public interface IDatabase
⋮----
/// <summary>
/// Get id's for words in [query]. [outIgnored] contains those word from query that is
/// not present in any document.
/// </summary>
List<int> GetWordIds(string[] query, out List<string> outIgnored);
List<BEDocument> GetDocDetails(List<int> docIds);
⋮----
/// Perform the essential search for documents. It will return
/// a list of KeyValuePairs - the key is the id of the
/// document, and value is the number of words from the query
/// contained in the document.
⋮----
List<KeyValuePair<int, int>> GetDocuments(List<int> wordIds);
⋮----
/// Return all id of words, contained in [wordIds], but not
/// present in the document with id [docId]
⋮----
List<int> getMissing(int docId, List<int> wordIds);
⋮----
/// Convert a list of word id's to a list of the value of the
/// words
⋮----
List<string> WordsFromIds(List<int> wordIds);
List<string> GetWordsMatchingPattern(string pattern);
Dictionary<int, List<string>> GetDocsWithMatchingWords(List<string> matchingWords);
</file>

<file path="ConsoleSearch/PatternDocumentHit.cs">
namespace ConsoleSearch
⋮----
public class PatternDocumentHit
</file>

<file path="ConsoleSearch/PatternSearchResult.cs">
namespace ConsoleSearch
⋮----
public class PatternSearchResult
</file>

<file path="ConsoleSearch/Program.cs">
namespace ConsoleSearch
⋮----
class Program
⋮----
static void Main(string[] args)
⋮----
new App().Run();
</file>

<file path="ConsoleSearch/SearchLogic.cs">
namespace ConsoleSearch
⋮----
public class SearchLogic
⋮----
IDatabase mDatabase;
⋮----
/* Perform search of documents containing words from query. The result will
         * contain details about amost maxAmount of documents.
         */
public SearchResult Search(String[] query)
⋮----
DateTime start = DateTime.Now;
// Convert words to wordids
var wordIds = mDatabase.GetWordIds(query, out ignored);
// perform the search - get all docIds
var docIds =  mDatabase.GetDocuments(wordIds);
// get ids for the first maxAmount
⋮----
int limit = Config.ResultLimit.HasValue ? Math.Min(Config.ResultLimit.Value, docIds.Count) : docIds.Count;
foreach (var p in docIds.GetRange(0, limit))
top.Add(p.Key);
// compose the result.
// all the documentHit
⋮----
foreach (var doc in mDatabase.GetDocDetails(top))
⋮----
var missing = mDatabase.WordsFromIds(mDatabase.getMissing(doc.mId, wordIds));
docresult.Add(new DocumentHit(doc, docIds[idx++].Value, missing));
⋮----
return new SearchResult(query, docIds.Count, docresult, ignored, DateTime.Now - start);
⋮----
public PatternSearchResult PatternSearch(string pattern)
⋮----
// Step 1: Find all words in the database that match the pattern
var matchingWords = mDatabase.GetWordsMatchingPattern(pattern);
⋮----
return new PatternSearchResult(new List<PatternDocumentHit>());
⋮----
// Step 2: Find which documents contain these words and which specific words are in each
var docsWithWords = mDatabase.GetDocsWithMatchingWords(matchingWords);
// Step 3: Apply the result limit BEFORE fetching full details
var limitedDocIds = docsWithWords.Keys.AsEnumerable();
⋮----
limitedDocIds = limitedDocIds.Take(Config.ResultLimit.Value);
⋮----
var finalDocIds = limitedDocIds.ToList();
// Step 4: Get the full details for the limited set of documents
var docDetails = mDatabase.GetDocDetails(finalDocIds);
var docDetailsMap = docDetails.ToDictionary(d => d.mId);
// Step 5: Assemble the final result
⋮----
foreach (var docId in finalDocIds) // Iterate over the limited list of IDs
⋮----
if (docDetailsMap.ContainsKey(docId))
⋮----
hits.Add(new PatternDocumentHit(docDetailsMap[docId], wordsInDoc));
⋮----
return new PatternSearchResult(hits);
</file>

<file path="ConsoleSearch/SearchResult.cs">
namespace ConsoleSearch
⋮----
/*
     * A data class representing the result of a search.
     * Hits is the total number of documents containing at least one word from the query.
     * DocumentHits is the documents and the number of words from the query contained in the document - see
     * the class DocumentHit
     * Ignored contains words from the query not present in the document base.
     * TimeUsed is the timespan used to perform the search.
     */
public class SearchResult
</file>

<file path="Documents/Modul 2 - Agenda/Assignemnts day 1.txt">
Agenda

1. Hvad er en søgemaskine?

Indeksering
Søgning
Datamodellen
2. Opgave 1 løses.

4. Code walk-through

Arkitektur
Indexer - crawler
console search
5. Opgave 2, 3, ... løses. Hvis I har andre opgaver, som I er nysgerrige på, så er I mere end velkommen til at kaste jer over dem. Del gerne jeres ideer med mig inden I går i krig. Måske jeg kan hjælpe jer i den rigtige retning eller forhindre I forsøger at løse for stor en opgave.

 

Opgaver

Opgave 1
Få søgemaskinen til at køre på jeres computer. I skal

Koden er sat op til at køre via .net 9.0. Sørg for at det er installeret på jeres computer.
Søgemaskinen anvender sqlite som database. Installer en sqlite browser - se https://sqlitebrowser.org/dl/Links to an external site..
Sørge for at builde alle projekter og eventuelt opdatere nuget packages.
Lægge nogle dokumenter i folderen til indexering. I kan selv lave nogle eller bruge nogle af dem i filen seData.zip.
Opdatere Shared.Path så stien til databasen peger på databasen.
Opdatere Indexer.Config så stien til hvor filerne der skal indekeres ligger er korrekt.
Kør indexer
Inspicér databasen, check at alle dokumenter er indexeret og lav nogle stikprøver mht hvilke ord der er indexeret.
Kør searchConsole. Afprøv med forespørgsler på først 1 ord, så 2 ord og tilsidst flere end 2 ord. Check både hit og ikke hit.
Opgave 2
Som det er nu, vil indekseren afslutte med at udskrive alle ord til konsollen. Output kan fx. se således ud:

DONE! used 986905.577
Indexed 50850 documents
Number of different words: 292796
The first 10 is:
<Message, 1>
<ID, 2>
<6184043, 3>
<1075857585289, 4>
<JavaMail, 5>
<evans, 6>
<thyme, 7>
<Date, 8>
<Tue, 9>
<28, 10>

Heraf fremgår det at processen med at indeksere de cirka 50.000 dokumenter tog knap 1000 sekunder (godt 15 min) og at der blev fundet knap 300.000 forskellige ord.

Lav det om så indekseringen afsluttes med, at der informeres om hvor mange ord forekomster der er indekseret og der spørges til hvor mange ord man vil se. Derefter vises det ønskede antal med deres hyppighed, men de ord der forekommer hyppigst, skal vises først. Så sidste det af output kunne være:

<Message, 1> - 48342
<ID, 2> - 47234
<mail, 12> - 47124
<the, 45> - 46847
... 

Opgave 3
I versionen er søgningen case sensitiv. Sørg for brugeren selv kan slå dette fra eller til. Det kan gøres ved, at ConsoleSearch ikke bare kan modtage queries, men også kommandoer, så brugeren kan indtaste "/casesensitive=on" hvis søgningerne skal være case sensitive (som de er nu), og indtaste "/casesensitive=off", hvis søgningerne ikke skal være case sensitive. Alternativt - eller til at starte med - kan I gøre det ved at oprette en Config klasse med en attribut med navn CaseSensitive af type boolean.

Opgave 4
Når man søger, får man info om tidspunkt for indeksering af dokumenter. Sørg for at brugeren vælger dette til eller fra. Det kan gøres ved brugeren kan skrive "/timestamp=on", hvis de skal med i resultatlisten eller "/timestamp=off", hvis det ikke skal med. Alternativt - eller til at starte med - kan I gøre det ved at have en attribut med navn ViewTimeStamps af type boolean i config.cs i Shared projektet.

Opgave 5
Som det er nu, vises de 10 "bedste" dokumenter som resultat af en søgning. Sæt dette til 20. Derefter, sørg for at brugeren kan vælge dette, fx ved kommandoen "/results=15" for at få de 15 bedste, eller "/results=all" for at få vist alle. Alternativt - eller til at starte med - kan I gøre det ved, at have en attribut i Config klassen, som reræsenterer denne mulige begrænsning. En simpel type for dette kunne være int? - altså en int, som kan være null. Her kunne "reglen" så være, at hvis den er null, skal man se alle resultater, ellers er dens værdi grænsen for hvor mange resultater der skal vises.

Opgave 6 - svær - måske i projektet?
Udvid søgemaskinen med mulighed for mønstersøgning. Forestil jer at domænet er politi og at de i forbindelse med en forbrydelse har en del af en nummerplade, fx. de ved den starter med BJ og ender på 7. De vil nu gerne finde alle steder hvor dette nummer kan være anført. Med såkaldte wildcard eller regulære udtryk kunne et mønster være følgende "BJ????7". Alle ord på 7 tegn, startende med BJ og slutter med 7 vil matche dette mønster. I et mønster kan bruges enten '?' som står for netop et vilkårligt tegn, mens '*' står for et vilkårligt antal (også ingen) tegn. Til at søge efter ovennævnte nummerplade, kunne man bruge et lidt simplere mønster, nemlig "BJ*7". Men det vil matche ord som bestemt ikke angiver en nummerplade. Hvorfor det?
Resultatet af en mønstersøgning skal være alle de dokumenter, som indeholder et ord, som matcher søgeordet. Derudover skal resultatet vise de ord fra dokumentet som der matches på. Derved kunne output komme til at se således ud:

enter search terms - q for quit
BJ????7
Pattern Search
1: /Users/peter/documents/2019/obs/05-12.txt -- contains 3 matching terms:
BJ12347, BJERGE7, BJ73857

2: /Users/John/documents/2017/contacts/12_17.txt -- contains 1 matching term:
BJ43567
I den første version af mønstersøgning er det en god idé at lave det, så der kun kan søges på et mønster og ikke i kombination med almindelige søgeord.
</file>

<file path="Documents/Modul 2 - Agenda/Database_Inspection_Guide.md">
# Database Inspektion & Søgetest Vejledning

Denne vejledning viser hvordan du inspicerer SQLite databasen og grundigt tester søgefunktionaliteten.

## 📊 Database Inspektion med SQLite Browser

### 1. Åbn Databasen
1. Start **DB Browser for SQLite**
2. **File → Open Database**
3. Naviger til: `/Users/rosell/ITA_SEM6_SearchEngine/Data/searchDB.db`
4. Klik **Open**

### 2. Verificer at Alle Dokumenter er Indekseret

#### Tjek Dokument Antal
1. Klik **Browse Data** fanen
2. Vælg **`document`** tabellen fra dropdown
3. Verificer at du ser **3.034 rækker** (for medium datasæt)
4. Scroll gennem for at se forskellige dokument stier

#### Eksempel Dokument Indgange
Led efter indgange som:
```
id: 1, url: /Users/rosell/.../allen-p/all_documents/1.txt, idxTime: 8/27/2025 12:01:54 PM
id: 2, url: /Users/rosell/.../allen-p/all_documents/2.txt, idxTime: 8/27/2025 12:01:54 PM
```

### 3. Inspicér Indekserede Ord (Stikprøver)

#### Tjek Ord Antal
1. Vælg **`word`** tabellen fra dropdown  
2. Verificer at du ser **31.079 rækker** (for medium datasæt)
3. Browse gennem ordene for at se hvad der blev indekseret

#### Eksempel Ord Indgange
Led efter indgange som:
```
id: 1, name: Message
id: 2, name: ID  
id: 5, name: JavaMail
id: 6, name: evans
```

#### Avanceret Ord Analyse med SQL
Klik **Execute SQL** fanen og kør disse forespørgsler:

```sql
-- Tæl totale dokumenter
SELECT COUNT(*) as total_documents FROM document;

-- Tæl totale unikke ord
SELECT COUNT(*) as total_words FROM word;

-- Tæl totale ord forekomster
SELECT COUNT(*) as total_occurrences FROM occ;

-- Find hyppigste ord
SELECT w.name, COUNT(*) as frequency
FROM word w
JOIN occ o ON w.id = o.wordId
GROUP BY w.name
ORDER BY frequency DESC
LIMIT 20;

-- Find dokumenter der indeholder specifikt ord
SELECT d.url, d.idxTime 
FROM document d
JOIN occ o ON d.id = o.docId  
JOIN word w ON o.wordId = w.id
WHERE w.name = 'energy'
ORDER BY d.idxTime;

-- Tjek om specifikke ord eksisterer
SELECT name FROM word 
WHERE name IN ('power', 'energy', 'meeting', 'email')
ORDER BY name;

-- Tilfældige dokumenter
SELECT * FROM document 
ORDER BY RANDOM() 
LIMIT 10;
```

### 4. Verificer Indeks Integritet

Tjek **`occ`** (occurrence) tabellen:
```sql
-- Verificer ord-dokument relationer
SELECT COUNT(*) as total_relationships FROM occ;

-- Eksempel ord-dokument forbindelser
SELECT w.name as word, d.url as document
FROM occ o
JOIN word w ON o.wordId = w.id
JOIN document d ON o.docId = d.id
LIMIT 20;
```

## 🔍 Søgekonsol Test

### 1. Start Søgekonsol
```bash
cd /Users/rosell/ITA_SEM6_SearchEngine/ConsoleSearch
dotnet run
```

### 2. Test Enkelt Ord Forespørgsler (1 ord)

#### Forventede Hit:
```
energy
```
**Forventet:** Flere resultater der viser dokumenter indeholdende "energy"

```
power
```
**Forventet:** Flere resultater fra strøm-relaterede emails

```
meeting
```
**Forventet:** Møde-relaterede dokumenter

#### Forventede Ingen Hit:
```
unicorn
```
**Forventet:** "Ignored: unicorn" og "Documents: 0"

```
supercalifragilisticexpialidocious
```
**Forventet:** "Ignored: [ord]" og "Documents: 0"

### 3. Test To Ord Forespørgsler (2 ord)

#### Begge Ord Fundet:
```
power plant
```
**Forventet:** Dokumenter indeholdende både "power" OG "plant", rangeret efter relevans

```
energy meeting
```
**Forventet:** Dokumenter indeholdende begge ord

#### Et Ord Mangler:
```
power unicorn
```
**Forventet:** 
- "Ignored: unicorn"
- Resultater indeholdende kun "power"
- "Missing: [unicorn]" i resultat detaljer

### 4. Test Multiple Ord Forespørgsler (flere end 2 ord)

#### Alle Ord Fundet:
```
power energy meeting
```
**Forventet:** Dokumenter indeholdende alle tre termer (højeste score først)

#### Blandede Resultater:
```
power energy unicorn dragon
```
**Forventet:**
- "Ignored: unicorn, dragon"
- Resultater med "power" og "energy"
- "Missing:" felt viser hvilke termer der mangler fra hvert dokument

#### Alle Ord Mangler:
```
unicorn dragon phoenix
```
**Forventet:** "Ignored: unicorn, dragon, phoenix" og "Documents: 0"

### 5. Forventet Output Format

For hvert søgeresultat skal du se:
```
1 : /sti/til/dokument.txt -- contains X search terms
Index time: 8/27/2025 12:01:54 PM
Missing: [liste af manglende termer]
```

Plus sammendrag:
```
Documents: 159. Time: 76.5
```

## 📋 Verifikations Tjekliste

### Database Integritet ✓
- [ ] Document tabel har 3.034 indgange (medium datasæt)
- [ ] Word tabel har ~31.079 indgange  
- [ ] Occurrence tabel har mange relationer
- [ ] Filstier i document tabel er korrekte
- [ ] Indeks tidsstempler er nylige

### Søgefunktionalitet ✓
- [ ] Enkelt ord søgning returnerer resultater
- [ ] Enkelt ord søgning håndterer "ingen resultater" elegant
- [ ] To ord søgning rangerer efter relevans (2 match > 1 match)
- [ ] Multiple ord søgning viser "Missing:" termer korrekt
- [ ] Ikke-eksisterende ord viser "Ignored:" besked
- [ ] Forespørgselstid er rimelig (< 200ms for de fleste forespørgsler)
- [ ] Resultater er begrænset til top 10
- [ ] Resultater viser filstier, match antal og tidsstempler

## 🎯 Eksempel Test Session

```
Console Search
enter search terms - q for quit

energy
[Skal vise ~75 resultater]

power plant  
[Skal vise dokumenter med begge termer først]

meeting email schedule
[Skal vise dokumenter rangeret efter hvor mange termer de indeholder]

unicorn
Ignored: unicorn
Documents: 0. Time: 46.615

q
[Afslut]
```

## 💡 Tips

1. **Tilfældig Prøvetagning**: Brug SQL forespørgsler til at vælge tilfældige dokumenter og verificer de er søgbare
2. **Ord Verifikation**: Vælg ord fra word tabellen og verificer de returnerer søgeresultater  
3. **Performance**: Bemærk søgetider - de skal være under 100ms for simple forespørgsler
4. **Edge Cases**: Test tomme forespørgsler, meget lange forespørgsler og specielle tegn
5. **Konsistens**: Kør den samme forespørgsel flere gange for at verificere konsistente resultater

Dette fuldfører database inspektion og søgetest kravene fra Opgave 1.
</file>

<file path="Documents/Modul 2 - Agenda/Intro to Case.txt">
Intro til case – en intern søgemaskine
Kontekst og krav
Brugerfladen
Indeksering og datamodel
Arkitektur
Walking Skeleton

Kontekst
En virksomhed/organisation med 50++ ansatte, som arbejder med og producerer dokumenter
Dokumenter indeholder ustruktureret data og findes i mange formater (docx, pdf, rtf, txt, csv, pptx, xlxs osv.)
Den samlede dokument samling er allokeret på mange fil servere på tilsammen flere terra-bytes.
Behovet er et søgesystem, som tilbyder “instant content search” og som kan tilpasses domænet.

Brugergrænsefladen
Brugergrænsefladen kan være en web applikation eller en desktop applikation.
Web applikationen vil være begrænset til intranettet.
En desktop applikation skal installeres på alle computere.
Brugeren vil have en Google-like brugerflade, hvor 
der indtastes søgeord (query)
Resultatet er en liste af hits med titel, link, dato, score og snippet.


Specielle Krav
100% recall – det betyder, at alle dokumenter, som indeholder mindst et af søgeordene, skal med i resultatet.
Rankeringen skal være med aftagene score.
Givet en søgning og et dokument, så er scoren et tal mellem 0 og 1. 
0 hvis ingen af søgeordene er i dokumentet
1 hvis alle søgeordene er i dokumentet.
Hvis x% af søgeordene er i dokumentet, så er scoren x/100.

Tilpasse domæne
Hvert domæne (ingeniør, forsikring, politi, meteologi…) har nogle specifikke betegnelser/slang.
Det skal være muligt at søge efter (domæne specifikke) synonymer.
Dette kan gøres ved, at søgemaskinen gør brug af en synonym ordbog (termnet), således at der også søges på synonymer.
Det konkrete indhold af synonym ordbogen kan vedligeholdes af brugerne (domænet).


Indeksering
For at kunne udføre indholdssøgning i millioner af dokumenter effektivt, kan man indeksere dokumenterne.
En indekser gennemlæser dokumenter (eng: crawl) og for hvert af disse vil det udtrække ord og danne et omvendt indeks.
Et omvendt indeks tilbyder effektiv søgning for dokumenter som indeholder bestemte ord.

En begrebsmæssig datamodel for det omvendte indeks.
Document
Title
Link
Date
Word
value
Occurrence
*
*
Filsystemet kan hurtigt svare på hvilke ord et givet dokument indeholder. 
Behovet er dog hurtige svar på det omvendte: 

                          Hvilke dokumenter indeholder et givet ord?

Walking Skeleton (version 1)Funktionalitet
Søgningen
er et konsol program indeholdende søgelogikken
Ingen snippets
Ingen synonym ordbog
Indekser
er et konsol program og kan indeksere bare en top-folder 
Kan kun indeksere .txt dokumenter
kan ikke opdatere – laver helt nyt indeks når den startes
Databasen
En simpel SQLite database med 3 tabeller: Document, Word, og Occurrence.

En relationel datamodel for det omvendte indeks
Document
docId
Title
Link
Date
Term
termId
value
Occurrence
docId
termId

Walking Skeleton (version 1)Arkitektur – class diagram

Walking Skeleton (version 1)Arkitektur - deployment

Arkitektur – version 2
Search programDesktop
Search Engine(API)
Database
Indexer
File system
Search programWebApp

Class diagram – version 2
</file>

<file path="Documents/Modul 2 - Agenda/Test Data explanation.txt">
Testdata
En meget vigtig del af udviklingen af søgemaskinen er, at have styr på sine test data. Uden dem, kan man slet ikke afprøve hvordan den virker i produktion.

Til søgemaskinen har man brug for mange filer med et realistisk indhold. Til det formål er enronLinks to an external site. datasættet helt perfekt, og er da også blevet brugt til rigtig meget. Det er et snapshot af en mail-server, hvor hver mail er skrevet i en fil, og hvor hver mailbox er en folder. Ligeledes er hver bruger også en folder. Se figur herunder.

Screenshot 2024-08-30 at 10.56.41.png

 

Filstruktur for mailserver

Der er lavet tre datasæt i files seData.zip, som kan hentes her Download her(fylder 100mb). De tre datasæt:
** SE MIG **
// Jeg har gjort dette, alt ligger i: C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data

small - et lille antal (13) mails i en folder. Kan bruges til funktionel test.
Medium - cirka 5000 mails i cirka 20 mailbokse for en bruger. Kan bruges til funktionel test - og en start på performance test.
Large - cirka 50000 mails for 15 brugere. Kan bruges til performance test.
Derudover er selvfølgelig også hele datasættet fra enron som indeholder cirka 500.000 mails.
</file>

<file path="Documents/Modul 3 - Agenda/Læselektier.txt">
AKF Scale Cube - Skaleringsarkitektur
Kapitel 22: Introduktion til AKF Scale Cube
Grundlæggende koncept
AKF Scale Cube er en 3-dimensionel model til skalering af systemer, organisationer og processer. Modellen starter fra et monolitisk udgangspunkt (0,0,0) - typisk et enkelt system på en enkelt server - og ekspanderer langs tre akser, hver repræsenterende en unik skaleringsmetode.
  

De tre skalerings-dimensioner
X-aksen: Kloning og replikering
* Princip: Kloning af services/data uden bias - arbejde distribueres ligeligt mellem identiske kopier
* Fordele: Simpel implementering, lav omkostning, hurtig udrulning
* Ulemper: Skalerer ikke godt med voksende datamængder eller instruktionssæt
* Anvendelse: Ideel som første skridt i skalering når transaktionsvolumen vokser
Y-aksen: Funktionel opdeling
* Princip: Opdeling efter ansvar, handling eller datatype (service-orienterede splits)
* Fordele: Håndterer kompleksitet, reducerer instruktionssæt, forbedrer fejlisolering
* Ulemper: Højere implementeringsomkostning end x-akse, kræver redesign
* Anvendelse: Nødvendig når monolitiske systemer bliver for komplekse
Z-aksen: Kunde-segmentering
* Princip: Opdeling baseret på kunde/anmoder med bias mod specifikke brugergrupper
* Fordele: Fremragende skalerbarhed, geografisk distribution mulig, kundespecifik optimering
* Ulemper: Ofte dyreste at implementere, kræver lookup-mekanismer
* Anvendelse: Kritisk for virksomheder med hurtig kundevækst
  

Kapitel 23: Splitting Applications for Scale
Applikations-specifik implementering
Når AKF Scale Cube anvendes på applikationer, får hver akse mere specifik betydning:
  

X-akse for applikationer
* Horisontal duplikering/kloning af services
* Load balancing mellem identiske instanser
* Ideel til håndtering af transaktionsvækst
* Udfordringer med stateful applikationer kræver session management
Y-akse for applikationer
* Split efter funktion eller service (verb-baseret opdeling)
* Adresserer monolitisk arkitektur gennem microservices
* Forbedrer udviklingshastighed og team-specialisering
* Muliggør uafhængig skalering af forskellige funktioner
Z-akse for applikationer
* Kunde/anmoder-orienterede splits med lookup-baseret routing
* Fremragende til kundevækst og data-partitionering
* Muliggør differentierede serviceniveauer (fx freemium modeller)
* Understøtter geografisk distribution af services
Praktisk integration af akser
Kombinationen af akser skaber robust og skalerbar arkitektur. Nedenstående eksempel viser hvordan kunde-segmentering (z-akse) kombineres med redundans (x-akse) for at opnå både skalerbarhed og høj tilgængelighed:
  

Hver kundegruppe (A-F, G-N, O-Z) har sin egen pod med multiple instanser, hvilket sikrer både performance og tilgængelighed.
Real-world implementering: PROS Airline System
PROS systemet demonstrerer kraftfuld anvendelse af alle tre akser i et komplekst produktionsmiljø:
  

Y-akse splits i PROS
* Origin & Destination (O/D) model separeret fra Real-Time Dynamic Pricing (RTDP)
* Inventory Stream Service som selvstændig komponent
* Cache Distributor håndterer data-distribution
Z-akse splits i PROS
* Forskellige Global Distribution Systems (GDS) betjenes af dedikerede RTDP instanser
* Hver GDS får sin egen "swim lane" for fault isolation
X-akse splits i PROS
* DB Clusters med N1-N5 noder for redundans
* Multiple Availability Services
* Load-balancerede instanser inden for hver service
Integration og Best Practices
Design vs. Implementering
* Design for alle tre akser tidligt i produktudviklingen
* Implementer kun når forretningsbehov kræver det
* Start typisk med x-akse, tilføj y- og z-akse efter behov
Hierarkisk struktur
* X-akse ofte underordnet Y eller Z
* Y- og Z-akse kan være primære splits med x-akse for redundans
* Vælg primær akse baseret på største skalerings-udfordring
Skalerings-strategi
1. Transaktionsvækst: Start med x-akse kloning
2. Kompleksitetsvækst: Tilføj y-akse service splits
3. Kundevækst: Implementer z-akse segmentering
4. Kombiner efter behov: Brug alle tre for "near infinite scale"
Nøglepunkter
X-akse karakteristika
* Styrker: Simpel, billig, hurtig at implementere
* Svagheder: Begrænset af data- og kodekompleksitet
* Use case: Første skridt i skalering, transaktionsvolumen
Y-akse karakteristika
* Styrker: Håndterer kompleksitet, forbedrer teamproduktivitet
* Svagheder: Kræver arkitekturændringer, dyrere end x-akse
* Use case: Feature-rige applikationer, microservices
Z-akse karakteristika
* Styrker: Ekstrem skalerbarhed, kundespecifik optimering
* Svagheder: Mest kompleks og dyr at implementere
* Use case: Hurtig kundevækst, geografisk distribution
Opsummering
AKF Scale Cube tilbyder en struktureret og konceptuel ramme for systemskalering gennem tre komplementære dimensioner. X-aksen håndterer transaktionsvolumen gennem simpel replikering, Y-aksen adresserer kompleksitet gennem funktionel opdeling, og Z-aksen muliggør ekstrem skalering gennem kunde-segmentering.
Succesfuld anvendelse kræver forståelse for at hver akse løser forskellige skalerings-udfordringer og har forskellige omkostningsprofiler. Ved at designe for alle tre dimensioner tidligt, men implementere progressivt baseret på faktiske behov, kan organisationer opnå effektiv skalering uden overinvestering i infrastruktur.
PROS eksemplet illustrerer hvordan moderne systemer kan håndtere ekstreme krav til både volumen og tilgængelighed gennem intelligent kombination af alle tre skaleringsmetoder, resulterende i systemer der kan betjene millioner af transaktioner med høj pålidelighed.
Vigtige læringspunkter
* Design tidligt, implementer sent: Planlæg for alle tre akser, men byg kun hvad der er nødvendigt
* Start simpelt: X-akse er ofte det bedste første skridt
* Kombiner strategisk: Forskellige akser løser forskellige problemer
* Fokus på behov: Lad faktiske skalerings-udfordringer drive implementering
* Mål og juster: Brug produktionsdata til at informere skalerings-beslutninger


________________
</file>

<file path="Documents/Modul 3 - Agenda/Opgaver_Modul3.txt">
# Opgaver Modul 3 - AKF Scale Cube
## IT-Arkitektur Semester 6, Erhvervsakademiet Aarhus

---

## Opgave 1: Webshop Skalering

**Forestil jer en webshop, hvor indkøbskurven (som er tilstand) – ligger på web-serveren. Webshoppen performer dårligt og man tænker på at x-skalere webshoppen.**

### Problemer:
- **Session Affinity Problem**: Når indkøbskurven ligger på web-serveren, skal brugeren altid ramme samme server-instans. Dette eliminerer fordelene ved X-akse skalering.
- **Load Balancer Kompleksitet**: Kræver "sticky sessions" eller session affinity, hvilket komplicerer load balancing og kan skabe ubalance.
- **Single Point of Failure**: Hvis en server går ned, mister brugerne deres indkøbskurve.
- **Ineffektiv Resource Udnyttelse**: Nogle servere kan være overbelastede mens andre står ledige, fordi brugere er "bundet" til specifikke servere.

### Løsninger:
1. **Eksternalisér Session State** (X-akse forbedring):
   - Flyt indkøbskurven til ekstern storage (Redis, database)
   - Gør alle web-servere stateless
   - Muliggør ægte load balancing mellem identiske instanser

2. **Database/Cache Skalering**:
   - Implementér distributed cache (Redis Cluster)
   - Database replication for session storage

3. **Forberedelse til Y-akse**:
   - Overvej at separere indkøbskurv-funktionalitet som egen microservice
   - Shopping Cart API som separat komponent

---

## Opgave 2: Billetnet.dk Skalering

**Billetnet.dk sælger billetter til større arrangementer. Salget er meget stort i peaks – som er forudsigelige. Fx 20.000 solgte billetter på bare 20 min., med op til 30.000 besøgende. Udenfor peaks er der ikke mange besøgende/salg.**

### Skalerings-strategier afhængig af bottleneck:

#### Hvis problemet er i Web-serveren (Præsentation):
**X-akse skalering:**
- Auto-scaling af web-server instanser
- Load balancer foran multiple web-servere
- CDN for statisk content (billeder, CSS, JS)
- Horizontal scaling under peaks

#### Hvis problemet er i Application-serveren (API):
**X-akse skalering:**
- Multiple API instanser med load balancing
- Container orchestration (Kubernetes) for auto-scaling

**Y-akse skalering:**
- Separer funktioner i microservices:
  - Ticket Search Service
  - Payment Processing Service
  - User Management Service
  - Event Management Service
- Queue-baseret arkitektur for ticket reservationer

#### Hvis problemet er i Databasen:
**X-akse database skalering:**
- Read replicas for søgninger
- Write scaling gennem clustering

**Z-akse database skalering:**
- Partitionering efter arrangement/event
- Geografisk distribution af data

#### Kombineret Strategi (Anbefalet):
1. **X-akse**: Auto-scaling af alle lag under peaks
2. **Y-akse**: Microservices for kritiske funktioner
3. **Z-akse**: Event-baseret partitionering
4. **Queue System**: Async processing af payments
5. **Caching**: Redis for populære events og user sessions

---

## Opgave 3: Y-skalering af ConsoleSearch

**Vi skal have y-skaleret ConsoleSearch, så den opdeles i to komponenter: en konsol applikation som varetager interaktion med brugeren (og kun dette) og et API som indeholder søgelogikken.**

### Arkitektur Design:

#### Komponent 1: Console Client
**Ansvar:** Kun brugerinteraktion
- Menu-system og bruger input
- Formatering og visning af resultater
- Configuration management (lokalt)
- HTTP client til API kommunikation

**Implementering:**
```
ConsoleSearch/
├── Program.cs          (Entry point)
├── ConsoleApp.cs       (UI logic, menu system)
├── ApiClient.cs        (HTTP kommunikation)
├── ResultFormatter.cs  (Display logic)
└── Config.cs          (UI preferences)
```

#### Komponent 2: Search API
**Ansvar:** Søgelogik og data access
- SearchLogic klassen flyttes hertil
- Database adgang
- Search algoritme og ranking
- RESTful endpoints

**Implementering:**
```
SearchAPI/
├── Program.cs              (Web API startup)
├── Controllers/
│   └── SearchController.cs (REST endpoints)
├── Services/
│   └── SearchService.cs    (SearchLogic klasse flyttet)
├── Models/
│   ├── SearchRequest.cs
│   └── SearchResponse.cs
└── Data/
    └── DatabaseSqlite.cs   (Data access)
```

#### API Endpoints:
```
GET /api/search?query={query}&limit={limit}&caseSensitive={bool}&pattern={bool}
POST /api/search (for komplekse queries)
GET /api/health
```

#### Kommunikation:
- Console app sender HTTP requests til API
- JSON serialization af requests/responses
- Error handling og timeout management

### Fordele ved Y-akse split:
- **Separation of Concerns**: UI og business logic adskilt
- **Skalerbarhed**: API kan skaleres uafhængigt
- **Multi-client Support**: Andre klienter kan bruge samme API
- **Deployment Flexibility**: Komponenter kan deployes separat
- **Team Specialization**: UI og backend teams kan arbejde parallelt

---

## Opgave 4: Web-app med Blazor

**Vi skal have lavet en web-app (brug gerne Blazor) til søgning, som anvender det API som blev lavet i opgave 3.**

### Blazor Server App Arkitektur:

#### Hovedkomponenter:

**1. Search Page Component**
```razor
@page "/search"
<SearchComponent />
```

**2. Search Component**
- Input felt til søge-queries
- Checkboxes for search options (case sensitive, pattern matching)
- Results display area
- Paging controls

**3. Services Integration**
```csharp
@inject ISearchService SearchService
```

#### Implementering Struktur:
```
BlazorSearchApp/
├── Program.cs
├── Pages/
│   ├── Index.razor         (Landing page)
│   └── Search.razor        (Main search page)
├── Components/
│   ├── SearchBox.razor     (Search input component)
│   ├── SearchResults.razor (Results display)
│   └── SearchOptions.razor (Configuration toggles)
├── Services/
│   ├── ISearchService.cs   (Interface)
│   └── SearchService.cs    (HTTP client til API)
├── Models/
│   ├── SearchRequest.cs    (Request DTOs)
│   └── SearchResult.cs     (Response DTOs)
└── wwwroot/
    ├── css/                (Styling)
    └── js/                 (Client-side scripts)
```

#### Key Features:

**Real-time Search:**
- Blazor Server's SignalR connection for responsive UI
- Debounced input for performance
- Loading indicators under søgning

**Search Options:**
- Toggle switches for case sensitivity
- Pattern matching enable/disable
- Result limit configuration
- Compact/detailed view toggle

**Results Display:**
- Paginated results
- Highlighting af search terms
- Document metadata (title, date, score)
- Responsive design for mobile/desktop

#### API Integration:
```csharp
public class SearchService : ISearchService
{
    private readonly HttpClient _httpClient;
    
    public async Task<SearchResponse> SearchAsync(SearchRequest request)
    {
        // HTTP call til Search API fra opgave 3
        var response = await _httpClient.PostAsJsonAsync("/api/search", request);
        return await response.Content.ReadFromJsonAsync<SearchResponse>();
    }
}
```

#### Fordele ved Blazor Web App:
- **Responsive UI**: Moderne web interface
- **Real-time Updates**: SignalR for live updates
- **Component Reusability**: Genbrugelige UI komponenter
- **Type Safety**: Shared DTOs mellem API og client
- **SEO Friendly**: Server-side rendering
- **Cross Platform**: Virker på alle browsere/enheder

### Integration Flow:
1. **User Input** → Blazor Component
2. **Search Request** → SearchService (HTTP client)
3. **API Call** → Search API (fra opgave 3)
4. **Database Query** → SearchLogic i API
5. **Results** → JSON response tilbage gennem stack
6. **UI Update** → Blazor re-renders results

Denne arkitektur demonstrerer fuld Y-akse skalering: UI lag (Blazor), API lag (Search API), og data lag (Database), hver med deres specifikke ansvar og skaleringsmuligheder.
</file>

<file path="indexer/App.cs">
namespace Indexer
⋮----
public class App
⋮----
public void Run(string dataset){
DatabaseSqlite db = new DatabaseSqlite(Paths.DATABASE);
Crawler crawler = new Crawler(db);
var root = new DirectoryInfo(Config.GetFolder(dataset));
DateTime start = DateTime.Now;
crawler.IndexFilesIn(root, new List<string> { ".txt"});
TimeSpan used = DateTime.Now - start;
Console.WriteLine("DONE! used " + used.TotalMilliseconds);
Console.WriteLine($"Indexed {db.DocumentCounts} documents");
Console.WriteLine($"Total word occurrences: {db.GetTotalOccurrences()}");
Console.WriteLine("How many of the most frequent words do you want to see?");
string input = Console.ReadLine();
if (int.TryParse(input, out int count) && count > 0)
⋮----
var frequentWords = db.GetMostFrequentWords(count);
Console.WriteLine($"The top {count} most frequent words are:");
⋮----
Console.WriteLine($"<{p.Item1}, {p.Item2}> - {p.Item3}");
</file>

<file path="indexer/Config.cs">
public class Config
⋮----
// Returns the folder to be indexed based on dataset size
// All .txt files in that folder (and subfolders) will be indexed
public static string GetFolder(string dataset)
⋮----
if (RuntimeInformation.IsOSPlatform(OSPlatform.Windows))
⋮----
return dataset.ToLower() switch
⋮----
_ => throw new ArgumentException($"Invalid dataset '{dataset}'. Valid options: small, medium, large")
⋮----
// macOS/Linux
</file>

<file path="indexer/Crawler.cs">
namespace Indexer
⋮----
public class Crawler
⋮----
private readonly char[] separators = " \\\n\t\"$'!,?;.:-_**+=)([]{}<>/@&%€#".ToCharArray();
/* Will be used to spilt text into words. So a word is a maximal sequence of
         * chars that does not contain any char from separators */
⋮----
/* Will contain all words from files during indexing - thet key is the 
         * value of the word and the value is its id in the database */
⋮----
/* Will count the number of documents indexed during indexing */
IDatabase mdatabase;
⋮----
//Return a dictionary containing all words (as the key)in the file
// [f] and the value is the number of occurrences of the key in file.
private ISet<string> ExtractWordsInFile(FileInfo f)
⋮----
var content = File.ReadAllLines(f.FullName);
⋮----
foreach (var aWord in line.Split(separators, StringSplitOptions.RemoveEmptyEntries))
⋮----
res.Add(aWord);
⋮----
private ISet<int> GetWordIdFromWords(ISet<string> src) {
⋮----
res.Add(words[p]);
⋮----
// Return a dictionary of all the words (the key) in the files contained
// in the directory [dir]. Only files with an extension in
// [extensions] is read. The value part of the return value is
// the number of occurrences of the key.
public void IndexFilesIn(DirectoryInfo dir, List<string> extensions) {
Console.WriteLine($"Crawling {dir.FullName}");
foreach (var file in dir.EnumerateFiles())
if (extensions.Contains(file.Extension))
⋮----
BEDocument newDoc = new BEDocument{
⋮----
mIdxTime = DateTime.Now.ToString(),
mCreationTime = file.CreationTime.ToString()
⋮----
mdatabase.InsertDocument(newDoc);
⋮----
if (!words.ContainsKey(aWord)) {
words.Add(aWord, words.Count + 1);
newWords.Add(aWord, words[aWord]);
⋮----
mdatabase.InsertAllWords(newWords);
mdatabase.InsertAllOcc(newDoc.mId, GetWordIdFromWords(wordsInFile));
⋮----
foreach (var d in dir.EnumerateDirectories())
</file>

<file path="indexer/DatabaseSqlite.cs">
namespace Indexer
⋮----
public class DatabaseSqlite : IDatabase
⋮----
private SqliteConnection _connection;
⋮----
var connectionStringBuilder = new SqliteConnectionStringBuilder();
⋮----
_connection = new SqliteConnection(connectionStringBuilder.ConnectionString);
_connection.Open();
⋮----
private void Execute(string sql)
⋮----
var cmd = _connection.CreateCommand();
⋮----
cmd.ExecuteNonQuery();
⋮----
public void InsertAllWords(Dictionary<string, int> res)
⋮----
using (var transaction = _connection.BeginTransaction())
⋮----
var command = _connection.CreateCommand();
⋮----
var paramName = command.CreateParameter();
⋮----
command.Parameters.Add(paramName);
var paramId = command.CreateParameter();
⋮----
command.Parameters.Add(paramId);
// Insert all entries in the res
⋮----
command.ExecuteNonQuery();
⋮----
transaction.Commit();
⋮----
public void InsertAllOcc(int docId, ISet<int> wordIds)
⋮----
var paramwordId = command.CreateParameter();
⋮----
command.Parameters.Add(paramwordId);
var paramDocId = command.CreateParameter();
⋮----
command.Parameters.Add(paramDocId);
⋮----
public void InsertWord(int id, string value)
⋮----
var insertCmd = new SqliteCommand("INSERT INTO word(id, name) VALUES(@id,@name)");
⋮----
var pName = new SqliteParameter("name", value);
insertCmd.Parameters.Add(pName);
var pCount = new SqliteParameter("id", id);
insertCmd.Parameters.Add(pCount);
insertCmd.ExecuteNonQuery();
⋮----
public void InsertDocument(BEDocument doc)
⋮----
new SqliteCommand(
⋮----
var pId = new SqliteParameter("id", doc.mId);
insertCmd.Parameters.Add(pId);
var pUrl = new SqliteParameter("url", doc.mUrl);
insertCmd.Parameters.Add(pUrl);
var pIdxTime = new SqliteParameter("idxTime", doc.mIdxTime);
insertCmd.Parameters.Add(pIdxTime);
var pCreationTime = new SqliteParameter("creationTime", doc.mCreationTime);
insertCmd.Parameters.Add(pCreationTime);
⋮----
public Dictionary<string, int> GetAllWords()
⋮----
var selectCmd = _connection.CreateCommand();
⋮----
using (var reader = selectCmd.ExecuteReader())
⋮----
while (reader.Read())
⋮----
var id = reader.GetInt32(0);
var w = reader.GetString(1);
res.Add(w, id);
⋮----
public long GetTotalOccurrences()
⋮----
return (long)selectCmd.ExecuteScalar();
⋮----
public List<(string, int, int)> GetMostFrequentWords(int limit)
⋮----
selectCmd.Parameters.AddWithValue("@limit", limit);
⋮----
res.Add((reader.GetString(0), reader.GetInt32(1), reader.GetInt32(2)));
⋮----
if (reader.Read())
⋮----
var count = reader.GetInt32(0);
</file>

<file path="indexer/IDatabase.cs">
namespace Indexer
⋮----
public interface IDatabase
⋮----
//Get all words with key as the value, and the value as the id
Dictionary<string, int> GetAllWords();
// Return the number of documents indexed in the database
⋮----
void InsertDocument(BEDocument doc);
// Insert a word in the database with id = [id] and value = [value]
void InsertWord(int id, string value);
void InsertAllWords(Dictionary<string, int> words);
void InsertAllOcc(int docId, ISet<int> wordIds);
long GetTotalOccurrences();
List<(string, int, int)> GetMostFrequentWords(int limit);
</file>

<file path="indexer/indexer.csproj">
<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <OutputType>Exe</OutputType>
    <TargetFramework>net9.0</TargetFramework>
  </PropertyGroup>

  <ItemGroup>
    <None Remove="SQLitePCLRaw.core" />
    <None Remove="Microsoft.Data.Sqlite.Core" />
    <None Remove="Microsoft.Data.Sqlite" />
  </ItemGroup>
  <ItemGroup>
    <PackageReference Include="Microsoft.Data.Sqlite" Version="8.0.1" />
  </ItemGroup>
  <ItemGroup>
    <ProjectReference Include="..\Shared\Shared.csproj" />
  </ItemGroup>
</Project>
</file>

<file path="indexer/Program.cs">
namespace Indexer
⋮----
class Program
⋮----
static void Main(string[] args)
⋮----
dataset = args[0].ToLower();
⋮----
Console.WriteLine("Select dataset:");
Console.WriteLine("1. small  - 5 files (quick test)");
Console.WriteLine("2. medium - ~5,000 emails (functional + performance)");
Console.WriteLine("3. large  - ~50,000 emails (performance testing)");
Console.Write("Enter choice (small/medium/large): ");
string input = Console.ReadLine()?.ToLower().Trim();
⋮----
Console.WriteLine($"Invalid dataset '{dataset}'. Valid options: small, medium, large");
⋮----
Console.WriteLine($"Starting indexing with '{dataset}' dataset...");
new App().Run(dataset);
//new Renamer().Crawl(new DirectoryInfo(@"/Users/ole/data"));
</file>

<file path="indexer/Renamer.cs">
namespace Indexer
⋮----
public class Renamer
⋮----
public void Run()
⋮----
void RenameFile(FileInfo f)
⋮----
if  (f.FullName.EndsWith(".txt")) return;
if (f.Name.StartsWith('.')) return;
var ending = f.FullName.EndsWith(".") ? "txt" : ".txt";
File.Move(f.FullName, f.FullName + ending, true);
⋮----
public void Crawl(DirectoryInfo dir)
⋮----
Console.WriteLine("Crawling " + dir.FullName);
foreach (var file in dir.EnumerateFiles())
⋮----
foreach (var d in dir.EnumerateDirectories())
</file>

<file path="Shared/Model/BEDocument.cs">
public class BEDocument
⋮----
public String mUrl;
public String mIdxTime;
public String mCreationTime;
</file>

<file path="Shared/Paths.cs">
namespace Shared
⋮----
public class Paths
⋮----
if (RuntimeInformation.IsOSPlatform(OSPlatform.Windows))
⋮----
else if (RuntimeInformation.IsOSPlatform(OSPlatform.OSX))
⋮----
// Default to Linux/Unix style path
</file>

<file path="Shared/Shared.csproj">
<Project Sdk="Microsoft.NET.Sdk">

  <PropertyGroup>
    <TargetFramework>net9.0</TargetFramework>
  </PropertyGroup>

  <ItemGroup>
    <None Remove="Model\" />
  </ItemGroup>
  <ItemGroup>
    <Folder Include="Model\" />
  </ItemGroup>
</Project>
</file>

<file path=".gitattributes">
# Auto detect text files and perform LF normalization
* text=auto
</file>

<file path=".gitignore">
# globs
Makefile.in
*.userprefs
*.usertasks
config.make
config.status
aclocal.m4
install-sh
autom4te.cache/
*.tar.gz
tarballs/
test-results/

# Mac bundle stuff
*.dmg
*.app

# content below from: https://github.com/github/gitignore/blob/main/Global/macOS.gitignore
# General
.DS_Store
.AppleDouble
.LSOverride

# Icon must end with two \r
Icon


# Thumbnails
._*

# Files that might appear in the root of a volume
.DocumentRevisions-V100
.fseventsd
.Spotlight-V100
.TemporaryItems
.Trashes
.VolumeIcon.icns
.com.apple.timemachine.donotpresent

# Directories potentially created on remote AFP share
.AppleDB
.AppleDesktop
Network Trash Folder
Temporary Items
.apdisk

# content below from: https://github.com/github/gitignore/blob/main/Global/Windows.gitignore
# Windows thumbnail cache files
Thumbs.db
ehthumbs.db
ehthumbs_vista.db

# Dump file
*.stackdump

# Folder config file
[Dd]esktop.ini

# Recycle Bin used on file shares
$RECYCLE.BIN/

# Windows Installer files
*.cab
*.msi
*.msix
*.msm
*.msp

# Windows shortcuts
*.lnk

# content below from: https://github.com/github/gitignore/blob/master/VisualStudio.gitignore
## Ignore Visual Studio temporary files, build results, and
## files generated by popular Visual Studio add-ons.
##
## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore

# User-specific files
*.suo
*.user
*.userosscache
*.sln.docstates

# User-specific files (MonoDevelop/Xamarin Studio)
*.userprefs

# Build results
[Dd]ebug/
[Dd]ebugPublic/
[Rr]elease/
[Rr]eleases/
x64/
x86/
bld/
[Bb]in/
[Oo]bj/
[Ll]og/

# Visual Studio 2015/2017 cache/options directory
.vs/
# Uncomment if you have tasks that create the project's static files in wwwroot
#wwwroot/

# Visual Studio 2017 auto generated files
Generated\ Files/

# MSTest test Results
[Tt]est[Rr]esult*/
[Bb]uild[Ll]og.*

# NUNIT
*.VisualState.xml
TestResult.xml

# Build Results of an ATL Project
[Dd]ebugPS/
[Rr]eleasePS/
dlldata.c

# Benchmark Results
BenchmarkDotNet.Artifacts/

# .NET Core
project.lock.json
project.fragment.lock.json
artifacts/

# StyleCop
StyleCopReport.xml

# Files built by Visual Studio
*_i.c
*_p.c
*_h.h
*.ilk
*.meta
*.obj
*.iobj
*.pch
*.pdb
*.ipdb
*.pgc
*.pgd
*.rsp
*.sbr
*.tlb
*.tli
*.tlh
*.tmp
*.tmp_proj
*_wpftmp.csproj
*.log
*.vspscc
*.vssscc
.builds
*.pidb
*.svclog
*.scc

# Chutzpah Test files
_Chutzpah*

# Visual C++ cache files
ipch/
*.aps
*.ncb
*.opendb
*.opensdf
*.sdf
*.cachefile
*.VC.db
*.VC.VC.opendb

# Visual Studio profiler
*.psess
*.vsp
*.vspx
*.sap

# Visual Studio Trace Files
*.e2e

# TFS 2012 Local Workspace
$tf/

# Guidance Automation Toolkit
*.gpState

# ReSharper is a .NET coding add-in
_ReSharper*/
*.[Rr]e[Ss]harper
*.DotSettings.user

# JustCode is a .NET coding add-in
.JustCode

# TeamCity is a build add-in
_TeamCity*

# DotCover is a Code Coverage Tool
*.dotCover

# AxoCover is a Code Coverage Tool
.axoCover/*
!.axoCover/settings.json

# Visual Studio code coverage results
*.coverage
*.coveragexml

# NCrunch
_NCrunch_*
.*crunch*.local.xml
nCrunchTemp_*

# MightyMoose
*.mm.*
AutoTest.Net/

# Web workbench (sass)
.sass-cache/

# Installshield output folder
[Ee]xpress/

# DocProject is a documentation generator add-in
DocProject/buildhelp/
DocProject/Help/*.HxT
DocProject/Help/*.HxC
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/Html2
DocProject/Help/html

# Click-Once directory
publish/

# Publish Web Output
*.[Pp]ublish.xml
*.azurePubxml
# Note: Comment the next line if you want to checkin your web deploy settings,
# but database connection strings (with potential passwords) will be unencrypted
*.pubxml
*.publishproj

# Microsoft Azure Web App publish settings. Comment the next line if you want to
# checkin your Azure Web App publish settings, but sensitive information contained
# in these scripts will be unencrypted
PublishScripts/

# NuGet Packages
*.nupkg
# The packages folder can be ignored because of Package Restore
**/[Pp]ackages/*
# except build/, which is used as an MSBuild target.
!**/[Pp]ackages/build/
# Uncomment if necessary however generally it will be regenerated when needed
#!**/[Pp]ackages/repositories.config
# NuGet v3's project.json files produces more ignorable files
*.nuget.props
*.nuget.targets

# Microsoft Azure Build Output
csx/
*.build.csdef

# Microsoft Azure Emulator
ecf/
rcf/

# Windows Store app package directories and files
AppPackages/
BundleArtifacts/
Package.StoreAssociation.xml
_pkginfo.txt
*.appx

# Visual Studio cache files
# files ending in .cache can be ignored
*.[Cc]ache
# but keep track of directories ending in .cache
!*.[Cc]ache/

# Others
ClientBin/
~$*
*~
*.dbmdl
*.dbproj.schemaview
*.jfm
*.pfx
*.publishsettings
orleans.codegen.cs

# Including strong name files can present a security risk
# (https://github.com/github/gitignore/pull/2483#issue-259490424)
#*.snk

# Since there are multiple workflows, uncomment next line to ignore bower_components
# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)
#bower_components/

# RIA/Silverlight projects
Generated_Code/

# Backup & report files from converting an old project file
# to a newer Visual Studio version. Backup files are not needed,
# because we have git ;-)
_UpgradeReport_Files/
Backup*/
UpgradeLog*.XML
UpgradeLog*.htm
ServiceFabricBackup/
*.rptproj.bak

# SQL Server files
*.mdf
*.ldf
*.ndf

# Business Intelligence projects
*.rdl.data
*.bim.layout
*.bim_*.settings
*.rptproj.rsuser

# Microsoft Fakes
FakesAssemblies/

# GhostDoc plugin setting file
*.GhostDoc.xml

# Node.js Tools for Visual Studio
.ntvs_analysis.dat
node_modules/

# Visual Studio 6 build log
*.plg

# Visual Studio 6 workspace options file
*.opt

# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)
*.vbw

# Visual Studio LightSwitch build output
**/*.HTMLClient/GeneratedArtifacts
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
_Pvt_Extensions

# Paket dependency manager
.paket/paket.exe
paket-files/

# FAKE - F# Make
.fake/

# JetBrains Rider
.idea/
*.sln.iml

# CodeRush personal settings
.cr/personal

# Python Tools for Visual Studio (PTVS)
__pycache__/
*.pyc

# Cake - Uncomment if you are using it
# tools/**
# !tools/packages.config

# Tabs Studio
*.tss

# Telerik's JustMock configuration file
*.jmconfig

# BizTalk build output
*.btp.cs
*.btm.cs
*.odx.cs
*.xsd.cs

# OpenCover UI analysis results
OpenCover/

# Azure Stream Analytics local run output
ASALocalRun/

# MSBuild Binary and Structured Log
*.binlog

# NVidia Nsight GPU debugger configuration file
*.nvuser

# MFractors (Xamarin productivity tool) working folder
.mfractor/

# Local History for Visual Studio
.localhistory/\"Data/searchDB.db\"
</file>

<file path=".repomixignore">
Data/
</file>

<file path="assignments.md">
# SearchEngine PoC - Opgaver og Fremgang

## Oversigt
Denne fil holder styr på alle opgaver til SearchEngine projektet og vores fremgang med hver opgave.

****************************************************
**MODUL 2 - SØGEMASKINE OPGAVER**
****************************************************

## Opgave 1: Opsætning og Installation
**Status**: 🔄 I gang  
**Beskrivelse**: Få søgemaskinen til at køre på computeren

### Krav:
- [x] Koden kræver .net 9.0 - skal installeres
- [x] SQLite browser installation - se https://sqlitebrowser.org/dl/
- [x] Builde alle projekter og opdatere NuGet packages
- [x] Lægge dokumenter i indexering-folderen (brug seData.zip filer)
- [x] Opdatere `Shared.Paths` så database-stien peger rigtigt
- [x] Opdatere `Indexer.Config` så fil-stien til indeksering er korrekt
- [ ] Køre indexer programmet
- [ ] Inspicere databasen og lave stikprøver af indekserede ord
- [ ] Køre searchConsole og afprøve med 1 ord, 2 ord og flere ord

### 🔧 Sådan Kører Du Systemet:

**1. Build projektet:**
```bash
cd "C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main"
dotnet build
```

**2. Kør indexer (opretter database og indekserer filer):**
```bash
cd indexer
dotnet run
```

**3. Kør søgemaskinen:**
```bash
cd ConsoleSearch
dotnet run
```

**4. Inspicer databasen (SQLite Browser):**
- Åbn SQLite Browser (DB Browser for SQLite)
- File → Open Database
- Vælg: `C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data\searchDB.db`
- Se på tabellerne: `document`, `word`, `occ` (occurrence)
- Lav stikprøver - check at dokumenter og ord er indekseret korrekt

**5. Test søgninger:**
- Skriv et ord og tryk Enter
- Skriv flere ord adskilt af mellemrum
- Skriv `q` for at afslutte

### Nuværende Konfiguration (skal ændres):
```csharp
// Shared\Paths.cs
public static string DATABASE = @"/Users/oleeriksen/Data/searchDBmedium.db";

// indexer\Config.cs  
public static string FOLDER = @"/Users/oleeriksen/Data/seData/medium";
```

### ✅ Hvad Vi Har Gjort:
- [x] Analyseret hele projektet og forstået arkitekturen
- [x] Identificeret konfigurations-filer der skal opdateres
- [x] Opdateret `Shared\Paths.cs` med Windows database sti
- [x] Opdateret `indexer\Config.cs` med Windows folder sti
- [x] Buildet projekterne succesfuldt (0 warnings, 0 errors)
- [x] Kørt indexer og oprettet database (13MB, 3034 dokumenter, 31079 forskellige ord)
- [ ] Testet søgemaskinen med forskellige søgninger

### 📁 Opdaterede Windows Stier:
```csharp
// Shared\Paths.cs:
DATABASE = @"C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data\searchDB.db";

// indexer\Config.cs:
FOLDER = @"C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data\seData copy\medium";
```

### 🎉 Indexer Resultater:
```
DONE! used 111991,158 ms (1 minut 52 sekunder)
Indexed 3034 documents
Number of different words: 31079
Database size: 13 MB
```

**De første 10 ord i indekset:**
1. Message (ID: 1)
2. ID (ID: 2) 
3. 29790972 (ID: 3)
4. 1075855665306 (ID: 4)
5. JavaMail (ID: 5)
6. evans (ID: 6)
7. thyme (ID: 7)
8. Date (ID: 8)
9. Wed (ID: 9)
10. 13 (ID: 10)

### 🗃️ Database Struktur (3 tabeller):

**1. `document` tabel:**
- `id` (INTEGER PRIMARY KEY) - Unikt dokument ID
- `url` (TEXT) - Fil sti til dokumentet
- `idxTime` (TEXT) - Tidspunkt for indeksering
- `creationTime` (TEXT) - Dokumentets oprettelsestidspunkt

**2. `word` tabel:**
- `id` (INTEGER PRIMARY KEY) - Unikt ord ID  
- `name` (VARCHAR(50)) - Selve ordet

**3. `occ` (occurrence) tabel:**
- `wordId` (INTEGER) - Reference til word.id
- `docId` (INTEGER) - Reference til document.id
- Foreign keys + index på wordId for hurtig søgning

**Sådan inspiceres databasen:**
1. Åbn DB Browser for SQLite
2. Open Database → vælg `searchDB.db`
3. Klik på "Browse Data" tab
4. Vælg tabel for at se indhold:
   - `document`: Se alle 3034 indekserede dokumenter
   - `word`: Se alle 31079 forskellige ord 
   - `occ`: Se ord-dokument relationer (mange-til-mange)

============================================

## Opgave 2: Forbedret Statistik
**Status**: ⏳ Afventer opgave 1  
**Beskrivelse**: Ændre indexer output til at vise ord-hyppighed

### Krav:
- Informere om hvor mange ord forekomster der er indekseret
- Spørge brugeren hvor mange ord de vil se
- Vise ord rangeret efter hyppighed (hyppigste først)
- Output format: `<Message, 1> - 48342` (ord, id, hyppighed)

### Hvad Vi Har Gjort:
- [ ] Endnu ikke påbegyndt

---

## Opgave 3: Case Sensitivity Kontrol  
**Status**: ⏳ Afventer opgave 1  
**Beskrivelse**: Gør søgning case-sensitiv kontrollerbar

### Krav:
- Bruger kan skrive `/casesensitive=on` eller `/casesensitive=off`
- Alternativ: Config klasse med `CaseSensitive` boolean attribut

### Hvad Vi Har Gjort:
- [ ] Endnu ikke påbegyndt

---

## Opgave 4: Timestamp Visning
**Status**: ⏳ Afventer opgave 1  
**Beskrivelse**: Brugeren kan vælge om tidsstempel skal vises

### Krav:
- Kommandoer som `/timestamp=on` eller `/timestamp=off`
- Alternativ: `ViewTimeStamps` boolean i Config klasse

### Hvad Vi Har Gjort:
- [ ] Endnu ikke påbegyndt

---

## Opgave 5: Konfigurerbar Resultat Grænse
**Status**: ⏳ Afventer opgave 1  
**Beskrivelse**: Ændre fra faste 10 resultater til bruger-valgt antal

### Krav:
- Ændre fra 10 til 20 som standard
- Kommandoer som `/results=15` eller `/results=all`
- Alternativ: `int?` attribut i Config (null = alle resultater)

### Hvad Vi Har Gjort:
- [ ] Endnu ikke påbegyndt

---

## Opgave 6: Mønster-søgning (Avanceret)
**Status**: ⏳ Afventer opgave 1  
**Beskrivelse**: Wildcard/regulære udtryk søgning

### Krav:
- `?` = et vilkårligt tegn
- `*` = vilkårligt antal tegn (også ingen)
- Eksempel: `BJ????7` matcher 7-tegns ord startende med BJ, sluttende med 7
- Vis matchende termer i resultatet
- Første version: kun et mønster ad gangen (ikke kombineret med normale søgeord)

### Eksempel Output:
```
enter search terms - q for quit
BJ????7
Pattern Search
1: /path/to/document.txt -- contains 3 matching terms:
BJ12347, BJERGE7, BJ73857
```

### Hvad Vi Har Gjort:
- [ ] Endnu ikke påbegyndt

---

## Noter og Overvejelser

### Skole Projekt Begrænsninger:
- Vi må ikke nødvendigvis redigere alle filer i projektet
- Kan være begrænsninger i at oprette nye filer
- Simplicitet er med vilje - det er et uddannelsesprojekt

### Test Data:
- Small dataset: 13 emails (funktionel test)
- Medium dataset: ~5000 emails (funktionel + performance)  
- Large dataset: ~50000 emails (performance test)

### Vigtige Filer at Forstå:
- `indexer\App.cs` - Indeksering workflow
- `indexer\Crawler.cs` - Tekst udtrækning og ord parsing
- `ConsoleSearch\App.cs` - Søge interface
- `ConsoleSearch\SearchLogic.cs` - Søge algoritme
- `Shared\Paths.cs` - Konfiguration der skal opdateres
</file>

<file path="Claude.md">
# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

# SearchEngine PoC - Project Analysis

## Project Overview
This is a **Proof of Concept (PoC) Search Engine** for IT-Architecture semester 6 at Erhvervsakademiet Aarhus. The system is designed as an internal document search solution for organizations with 50+ employees.

**Important Note**: This is a school project with restrictions on file modifications and creation. Not all files may be editable, and the simplicity is intentional for educational purposes.

## Architecture & Components

### Solution Structure
- **`indexer`** - Console application that crawls and indexes documents
- **`ConsoleSearch`** - Console application providing search functionality  
- **`Shared`** - Class library containing common models and configuration

### Technology Stack
- **.NET 9.0** C# console applications
- **SQLite database** for inverted index storage
- **Microsoft.Data.Sqlite** NuGet package (version 8.0.1)

### Database Schema (Inverted Index)
SQLite database with three main tables:
```sql
Document: docId, title, link, date
Word: termId, value  
Occurrence: docId, termId (many-to-many relationship)
```

## Core Components Analysis

### 1. Indexer (`indexer` project)
**Entry Point**: `Program.cs` → `App.cs`
- **Configuration**: `Config.cs` - defines folder to index
- **Crawler**: `Crawler.cs` - main indexing logic
- **Database**: `DatabaseSqlite.cs` - handles database operations

**Key Implementation Details**:
- Only indexes `.txt` files recursively
- Word extraction separators: `" \\\n\t\"$'!,?;.:-_**+=)([]{}<>/@&%€#"`
- Creates inverted index: word → documents containing it
- Platform-specific database paths via `RuntimeInformation`

**Indexing Workflow**:
1. Prompts user to select dataset size (small/medium/large)
2. Recursively crawls configured directory for `.txt` files
3. Extracts and normalizes words using defined separators
4. Builds inverted index in SQLite database
5. Outputs comprehensive statistics including:
   - Total documents indexed
   - Total word occurrences
   - Top N most frequent words (user-configurable)

### 2. Search Engine (`ConsoleSearch` project)
**Entry Point**: `Program.cs` → `App.cs`
- **Search Logic**: `SearchLogic.cs` - implements search algorithm
- **Database**: `DatabaseSqlite.cs` - read-only database access
- **Models**: `SearchResult.cs`, `DocumentHit.cs`
- **Config**: `Config.cs` - feature toggles (case sensitivity, timestamps, result limits, pattern search, compact view)

**Search Workflow**:
1. Displays interactive menu with feature toggles
2. Accepts queries (normal search or pattern matching)
3. Maps query terms to word IDs in database
4. Finds intersecting documents using inverted index
5. Calculates relevance scores and ranks results
6. Returns configurable number of results (default 20) with metadata

**Scoring Algorithm**: 
```
score = (number_of_matching_terms / total_query_terms)
```

### 3. Shared Library (`Shared` project)
- **`BEDocument.cs`**: Document business entity model
- **`Paths.cs`**: Cross-platform database path configuration (auto-detects Windows/macOS/Linux)
- **`IDatabase.cs`**: Database interface (used by both indexer and search)

## Test Data Structure

### Enron Email Dataset
Located in: `C:\Users\Gusta\OneDrive\Dokumenter\GitHub\SearchEngine-main\Data\seData copy\`

**Three Dataset Sizes**:
- **small**: 13 emails in 1 folder (functional testing)
- **medium**: ~5,000 emails in ~20 mailboxes (functional + performance)
- **large**: ~50,000 emails for 15 users (performance testing)

**Data Format**: Email files with headers and content
```
Message-ID: <...>
Date: Wed, 13 Dec 2000 18:41:00 -0800 (PST)
From: sender@domain.com
To: recipient@domain.com
Subject: Email Subject
...email content...
```

## Requirements (from Danish Documentation)

### Business Context
- Organization with 50+ employees producing documents
- Documents in multiple formats (docx, pdf, rtf, txt, csv, pptx, xlsx)
- Multi-terabyte collections across file servers
- Need for "instant content search"
- Domain-specific customization capability

### Technical Requirements
- **100% recall**: All documents containing any search terms must appear
- **Ranking**: Results ordered by descending score
- **Scoring**: Percentage of query terms found in document (0.0 to 1.0)
- **Future features**: Synonym dictionary for domain-specific terms

### User Interface Requirements
- Google-like search interface
- Results showing: title, link, date, score, snippet
- Can be web application (intranet) or desktop application

## Essential Commands

### Build Solution
```bash
dotnet build SearchEngine.sln
```

### Run Projects
```bash
# Run indexer with dataset selection (crawls and indexes documents)
cd indexer
dotnet run small     # Index small dataset (13 emails)
dotnet run medium    # Index medium dataset (~5,000 emails) 
dotnet run large     # Index large dataset (~50,000 emails)
# Alternative: dotnet run (will prompt for dataset selection)

# Run search console (interactive search)
cd ConsoleSearch
dotnet run
```

### Restore Packages
```bash
dotnet restore
```

### Configuration Setup
**No manual configuration needed!** The system automatically detects your platform (Windows/macOS/Linux) and uses appropriate paths:
- `Shared/Paths.cs` - Database path (auto-detects platform)
- `indexer/Config.cs` - Dataset folders (auto-detects platform)

### Database Inspection
Use SQLite browser to inspect `Data/searchDB.db` after indexing.

### Manual Testing Approach
**No formal test framework** - this project uses manual testing with realistic datasets:
- **Small dataset**: 13 emails for functional verification
- **Medium dataset**: ~5,000 emails for functional + basic performance testing  
- **Large dataset**: ~50,000 emails for performance testing
- Test search functionality with 1-word, 2-word, and multi-word queries
- Verify interactive menu options and configuration toggles

### Available Commands in Search Console
- **Menu Options**: `1`, `2`, `3`, `4`, `5` - Toggle various settings
- **Help**: `?` - Display comprehensive help with current settings and examples
- **Quit**: `q` - Exit the application
- **Slash Commands**: 
  - `/casesensitive=on|off` - Toggle case sensitivity
  - `/timestamp=on|off` - Toggle timestamp display
  - `/results=NUMBER|all` - Set result limit
  - `/pattern=on|off` - Toggle pattern search mode
  - `/compact=on|off` - Toggle compact view display

## Cross-Platform Configuration
✅ **Automatic platform detection implemented** via `RuntimeInformation.IsOSPlatform()`:
- **Database paths**: Automatically selected based on detected OS
- **Dataset folders**: Platform-specific paths configured in `indexer/Config.cs`
- **No manual path changes needed** when switching between systems
- See `Shared/Paths.cs` and `indexer/Config.cs` for implementation details

## Available Assignments (from Danish docs)

### Assignment 1: Setup
- Install .NET 9.0
- Install SQLite browser
- Build all projects, update NuGet packages
- Update paths in `Shared.Paths` and `Indexer.Config`
- Run indexer, inspect database
- Test search console with 1, 2, and multiple word queries

### Assignment 2: Enhanced Statistics ✅ **COMPLETED**
- ✅ Shows total word occurrences indexed
- ✅ Prompts user for number of most frequent words to display
- ✅ Displays words ranked by frequency (most frequent first)
- ✅ Format: `<word, id> - frequency`

### Assignment 3: Case Sensitivity Control ✅ **COMPLETED**
- ✅ Interactive menu option "1" to toggle case sensitivity
- ✅ `Config.CaseSensitive` boolean controls behavior
- ✅ Default: case-sensitive search enabled

### Assignment 4: Timestamp Display Control ✅ **COMPLETED**
- ✅ Interactive menu option "2" to toggle timestamp display
- ✅ `Config.ViewTimeStamps` boolean controls display
- ✅ Default: timestamps shown in results

### Assignment 5: Result Limit Configuration ✅ **COMPLETED**
- ✅ Interactive menu option "3" to configure result limits
- ✅ Supports specific numbers (e.g., 15) or "all" for unlimited
- ✅ `Config.ResultLimit` int? property (default: 20)
- ✅ Changed from fixed 10 to configurable system

### Assignment 6: Pattern Matching ✅ **COMPLETED**
- ✅ Interactive menu option "4" to enable pattern search mode
- ✅ Supports `?` (single char) and `*` (multiple chars) wildcards
- ✅ `Config.PatternSearch` boolean toggle
- ✅ Shows matching terms in results

### Additional Enhancements ✅ **IMPLEMENTED**
- ✅ **Compact View** - Option "5" for clean, single-line result display
- ✅ **Help System** - Type `?` for comprehensive contextual help
- ✅ **Enhanced Commands** - Additional slash commands (`/compact=on|off`)

## Current State Assessment

### ✅ Working Components
- Basic indexing and search functionality
- Clean three-project architecture
- Realistic test data (Enron dataset)
- Proper inverted index implementation
- Score-based ranking
- Cross-platform support (Windows/macOS/Linux)
- Enhanced statistics with word frequency
- Case sensitivity control (`Config.CaseSensitive`)
- Configurable result limits (`Config.ResultLimit`)
- Timestamp display control (`Config.ViewTimeStamps`)
- Pattern matching with wildcards (`Config.PatternSearch`)
- Compact view for clean result display (`Config.CompactView`)
- Interactive menu system for feature toggles
- Comprehensive help system with contextual examples (accessible via `?`)

### ⚠️ Areas for Future Enhancement
- No snippets in search results yet
- No synonym dictionary support
- Could expand wildcard patterns beyond single/multiple character matching

### 🚫 School Project Constraints
- Limited ability to edit certain files
- Cannot create arbitrary new files
- Simplicity is intentional for educational purposes
- Focus on extending existing functionality vs. major architectural changes

## Assignment Progress Tracking

**📋 See `assignments.md` for detailed progress tracking in Danish**

The `assignments.md` file contains:
- All 6 assignments from the Danish documentation
- Current status and progress for each assignment
- Detailed steps and requirements
- What has been completed vs. what remains

**Note**: All core assignments (1-6) have been implemented, plus additional user experience enhancements. The system now includes enhanced statistics, configurable search options, pattern matching capabilities, compact view display, and a comprehensive help system. See `assignments.md` for detailed implementation status.

## Academic Course Context

### Current Module: IT-Architecture Semester 6
- **Module 2**: Search Engine PoC (completed assignments 1-6)
- **Module 3**: AKF Scale Cube architecture patterns
- Course materials located in `Documents/Modul X - Agenda/` folders
- Assignment responses and analysis files created as course progresses

### Course Materials Structure
- **Reading materials**: `Læselektier.txt` and `Læselektier.pdf` files
- **Assignment responses**: `Opgaver_ModulX.txt` files with Danish answers
- **Database guides**: Technical documentation for SQLite inspection

## Getting Started Checklist
1. Verify .NET 9.0 installation
2. ✅ Cross-platform paths already configured automatically
3. Install SQLite browser for database inspection
4. Build solution: `dotnet build SearchEngine.sln`
5. Run indexer with dataset selection: `cd indexer && dotnet run medium`
6. Test search functionality: `cd ConsoleSearch && dotnet run`
7. Explore interactive menu features (case sensitivity, timestamps, result limits, pattern search, compact view)
8. Try the help system by typing `?` to see all available options and examples

## Architecture Notes

### Key Files
- `indexer/App.cs` - Indexing main workflow at indexer:32
- `indexer/Crawler.cs` - Text extraction and word parsing logic
- `ConsoleSearch/App.cs` - Interactive search console interface
- `ConsoleSearch/SearchLogic.cs` - Search algorithm and ranking at ConsoleSearch:45
- `Shared/Paths.cs` - Cross-platform database path configuration at Shared:7

### Search Algorithm Details
The system implements a basic TF (term frequency) scoring model where each document's relevance score is calculated as the percentage of query terms found within it. The inverted index allows efficient lookup of documents containing specific terms, with results ranked by descending relevance score and limited by the configurable result limit (default: 20).

### Configuration Dependencies
Both applications use automatic cross-platform configuration:
- `Shared/Paths.cs` uses `RuntimeInformation` to detect platform and select appropriate database paths
- `indexer/Config.cs` uses `RuntimeInformation` to select appropriate dataset folder paths
- `ConsoleSearch/Config.cs` provides feature toggles for search behavior:
  - `CaseSensitive` - Case-sensitive search matching
  - `ViewTimeStamps` - Display document indexing timestamps  
  - `ResultLimit` - Maximum results to display (int? - null = unlimited)
  - `PatternSearch` - Wildcard pattern matching mode
  - `CompactView` - Clean single-line result display format

### Interactive Features
The search console (`ConsoleSearch/App.cs`) provides an interactive menu system allowing users to toggle:
1. **Case Sensitivity** - Enable/disable case-sensitive search
2. **Timestamp Display** - Show/hide document timestamps in results
3. **Result Limits** - Configure number of results (default 20, or "all")
4. **Pattern Search** - Enable wildcard matching with `?` (single char) and `*` (multiple chars)
5. **Compact View** - Clean display format removing long file paths, showing results as single lines

**Enhanced Help System**: Users can type `?` (or option `6`) to access comprehensive contextual help showing current settings, examples, and available commands.
</file>

<file path="README.txt">
Version 1: 04-08-2025

This codebase is s PoC seachengine that consist of two programs and a class library.

The two programs are the indexer (also called a crawler) and a search program. Both
are simple console programs.

The indexer will crawl a folder (in depth) and create a reverse index
in a database. It will only index text files with .txt as extension.

The search program (see the ConsoleSearch project) offers a query-based search
in the reverse index.

The class library Shared contains classes that are used by the indexer
and the ConsoleSearch. It contains:

- Paths containing a static path the database (used by both the indexer (write-only), and
the search program (read-only).
- BEDocument (BE for Business Entity) - a class representing a document.
</file>

<file path="SearchEngine.sln">
Microsoft Visual Studio Solution File, Format Version 12.00
# Visual Studio Version 16
VisualStudioVersion = 16.0.810.10
MinimumVisualStudioVersion = 10.0.40219.1
Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "indexer", "indexer\indexer.csproj", "{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}"
EndProject
Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "ConsoleSearch", "ConsoleSearch\ConsoleSearch.csproj", "{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}"
EndProject
Project("{2150E333-8FDC-42A3-9474-1A3956D46DE8}") = "Solution Items", "Solution Items", "{72D568E9-23A6-46FD-97F0-7411FC0B2C8D}"
	ProjectSection(SolutionItems) = preProject
		README.txt = README.txt
	EndProjectSection
EndProject
Project("{FAE04EC0-301F-11D3-BF4B-00C04F79EFBC}") = "Shared", "Shared\Shared.csproj", "{3551940F-5CAC-4EB7-8514-629705C73335}"
EndProject
Global
	GlobalSection(SolutionConfigurationPlatforms) = preSolution
		Debug|Any CPU = Debug|Any CPU
		Release|Any CPU = Release|Any CPU
	EndGlobalSection
	GlobalSection(ProjectConfigurationPlatforms) = postSolution
		{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}.Debug|Any CPU.ActiveCfg = Debug|Any CPU
		{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}.Debug|Any CPU.Build.0 = Debug|Any CPU
		{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}.Release|Any CPU.ActiveCfg = Release|Any CPU
		{7F4ADC70-20AE-423B-9D46-FABEE2E9F5D2}.Release|Any CPU.Build.0 = Release|Any CPU
		{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}.Debug|Any CPU.ActiveCfg = Debug|Any CPU
		{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}.Debug|Any CPU.Build.0 = Debug|Any CPU
		{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}.Release|Any CPU.ActiveCfg = Release|Any CPU
		{8DF99C5C-09E2-430F-8DAD-5052DF2A9076}.Release|Any CPU.Build.0 = Release|Any CPU
		{3551940F-5CAC-4EB7-8514-629705C73335}.Debug|Any CPU.ActiveCfg = Debug|Any CPU
		{3551940F-5CAC-4EB7-8514-629705C73335}.Debug|Any CPU.Build.0 = Debug|Any CPU
		{3551940F-5CAC-4EB7-8514-629705C73335}.Release|Any CPU.ActiveCfg = Release|Any CPU
		{3551940F-5CAC-4EB7-8514-629705C73335}.Release|Any CPU.Build.0 = Release|Any CPU
	EndGlobalSection
	GlobalSection(SolutionProperties) = preSolution
		HideSolutionNode = FALSE
	EndGlobalSection
	GlobalSection(ExtensibilityGlobals) = postSolution
		SolutionGuid = {0EBC1CCA-0BFD-4D41-AE7E-1157EA73535D}
	EndGlobalSection
EndGlobal
</file>

</files>
